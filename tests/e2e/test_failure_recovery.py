"""
éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ (C1-C3)

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€RecRadikoã®éšœå®³å¾©æ—§æ©Ÿèƒ½ã‚’åŒ…æ‹¬çš„ã«ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
- C1: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ
- C2: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ  
- C3: ãƒ—ãƒ­ã‚»ã‚¹éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ
"""

import pytest
import os
import time
import threading
import shutil
import tempfile
import signal
import subprocess
import socket
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import patch, Mock, MagicMock
import requests
import json
from collections import defaultdict

# RecRadikoãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from src.cli import RecRadikoCLI
from src.auth import RadikoAuthenticator, AuthInfo
from src.program_info import ProgramInfoManager, Program, Station
from src.streaming import StreamingManager, StreamInfo, StreamSegment
from src.recording import RecordingManager, RecordingJob, RecordingStatus
from src.file_manager import FileManager, FileMetadata
from src.scheduler import RecordingScheduler, RecordingSchedule, RepeatPattern, ScheduleStatus
from src.daemon import DaemonManager, DaemonStatus
from src.error_handler import ErrorHandler


@pytest.mark.e2e
@pytest.mark.failure_recovery
@pytest.mark.network_dependent
class TestFailureRecoveryC1:
    """C1: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
    
    def test_network_failure_recovery(self, temp_environment, test_config, 
                                    mock_external_services, resource_monitor, network_simulator):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
        resource_monitor.start_monitoring(interval=0.5)
        start_time = time.time()
        
        # éšœå®³å¾©æ—§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        recovery_metrics = {
            'network_failures_simulated': 0,
            'successful_recoveries': 0,
            'recovery_time_seconds': [],
            'failed_requests': 0,
            'successful_retries': 0,
            'timeout_errors': 0,
            'dns_failures': 0
        }
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with patch('src.auth.RadikoAuthenticator') as mock_auth_class, \
             patch('src.program_info.ProgramInfoManager') as mock_program_class, \
             patch('src.streaming.StreamingManager') as mock_stream_class, \
             patch('requests.get') as mock_requests_get, \
             patch('requests.post') as mock_requests_post:
            
            # ãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®š
            authenticator = Mock()
            program_manager = Mock()
            stream_manager = Mock()
            
            mock_auth_class.return_value = authenticator
            mock_program_class.return_value = program_manager
            mock_stream_class.return_value = stream_manager
            
            # ã‚·ãƒŠãƒªã‚ª1: æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆéšœå®³
            print("ğŸŒ ã‚·ãƒŠãƒªã‚ª1: æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆéšœå®³ãƒ†ã‚¹ãƒˆ")
            failure_start = time.time()
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä¾‹å¤–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            mock_requests_get.side_effect = [
                requests.exceptions.Timeout("Connection timeout"),
                requests.exceptions.Timeout("Connection timeout"),
                Mock(status_code=200, json=lambda: {"status": "ok"})  # 3å›ç›®ã§æˆåŠŸ
            ]
            
            # èªè¨¼å¾©æ—§ã®ãƒ†ã‚¹ãƒˆ
            authenticator.authenticate.side_effect = [
                Exception("Network timeout"),
                Exception("Network timeout"), 
                AuthInfo(auth_token="recovery_token", area_id="JP13", expires_at=time.time() + 3600, premium_user=False)
            ]
            
            # å¾©æ—§ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            recovery_attempts = 0
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    recovery_metrics['network_failures_simulated'] += 1
                    
                    # èªè¨¼ãƒªãƒˆãƒ©ã‚¤
                    auth_result = authenticator.authenticate()
                    if auth_result:
                        recovery_time = time.time() - failure_start
                        recovery_metrics['recovery_time_seconds'].append(recovery_time)
                        recovery_metrics['successful_recoveries'] += 1
                        recovery_metrics['successful_retries'] += 1
                        print(f"   âœ… èªè¨¼å¾©æ—§æˆåŠŸ (è©¦è¡Œ{attempt + 1}å›ç›®, {recovery_time:.2f}ç§’)")
                        break
                        
                except Exception as e:
                    recovery_metrics['failed_requests'] += 1
                    recovery_metrics['timeout_errors'] += 1
                    print(f"   âŒ èªè¨¼å¤±æ•— (è©¦è¡Œ{attempt + 1}å›ç›®): {e}")
                    time.sleep(0.1)  # ãƒªãƒˆãƒ©ã‚¤é–“éš”
            
            # ã‚·ãƒŠãƒªã‚ª2: DNSè§£æ±ºå¤±æ•—
            print("ğŸŒ ã‚·ãƒŠãƒªã‚ª2: DNSè§£æ±ºå¤±æ•—ãƒ†ã‚¹ãƒˆ")
            failure_start = time.time()
            
            # DNSä¾‹å¤–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            mock_requests_get.side_effect = [
                requests.exceptions.ConnectionError("Name or service not known"),
                requests.exceptions.ConnectionError("Name or service not known"),
                Mock(status_code=200, json=lambda: {"stations": []})
            ]
            
            # ç•ªçµ„æƒ…å ±å–å¾—ã®ãƒªãƒˆãƒ©ã‚¤ãƒ†ã‚¹ãƒˆ
            program_manager.fetch_station_list.side_effect = [
                Exception("DNS resolution failed"),
                Exception("DNS resolution failed"),
                [Station(id="TEST", name="ãƒ†ã‚¹ãƒˆãƒ©ã‚¸ã‚ª", ascii_name="TEST", area_id="JP13")]
            ]
            
            for attempt in range(max_retries):
                try:
                    recovery_metrics['network_failures_simulated'] += 1
                    
                    # ç•ªçµ„æƒ…å ±å–å¾—ãƒªãƒˆãƒ©ã‚¤
                    stations = program_manager.fetch_station_list()
                    if stations:
                        recovery_time = time.time() - failure_start
                        recovery_metrics['recovery_time_seconds'].append(recovery_time)
                        recovery_metrics['successful_recoveries'] += 1
                        recovery_metrics['successful_retries'] += 1
                        print(f"   âœ… ç•ªçµ„æƒ…å ±å–å¾—å¾©æ—§æˆåŠŸ (è©¦è¡Œ{attempt + 1}å›ç›®, {recovery_time:.2f}ç§’)")
                        break
                        
                except Exception as e:
                    recovery_metrics['failed_requests'] += 1
                    recovery_metrics['dns_failures'] += 1
                    print(f"   âŒ ç•ªçµ„æƒ…å ±å–å¾—å¤±æ•— (è©¦è¡Œ{attempt + 1}å›ç›®): {e}")
                    time.sleep(0.1)
            
            # ã‚·ãƒŠãƒªã‚ª3: ä¸å®‰å®šãªæ¥ç¶šçŠ¶æ…‹
            print("ğŸŒ ã‚·ãƒŠãƒªã‚ª3: ä¸å®‰å®šãªæ¥ç¶šçŠ¶æ…‹ãƒ†ã‚¹ãƒˆ")
            failure_start = time.time()
            
            # æ–­ç¶šçš„ãªæ¥ç¶šå¤±æ•—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            connection_responses = [
                requests.exceptions.ConnectionError("Connection broken"),
                Mock(status_code=500, text="Internal Server Error"),
                requests.exceptions.Timeout("Request timeout"),
                Mock(status_code=200, content=b"mock_stream_data"),
                requests.exceptions.ConnectionError("Connection reset"),
                Mock(status_code=200, content=b"mock_stream_data")
            ]
            
            mock_requests_get.side_effect = connection_responses
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¾©æ—§ã®ãƒ†ã‚¹ãƒˆ
            stream_manager.get_stream_url.side_effect = [
                Exception("Stream connection failed"),
                Exception("Stream unavailable"),
                Exception("Network unstable"),
                "https://example.com/stream.m3u8",
                Exception("Connection reset"),
                "https://example.com/stream.m3u8"
            ]
            
            successful_streams = 0
            total_attempts = 6
            
            for attempt in range(total_attempts):
                try:
                    recovery_metrics['network_failures_simulated'] += 1
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¥ç¶šãƒªãƒˆãƒ©ã‚¤
                    stream_url = stream_manager.get_stream_url(
                        station_id="TEST",
                        program_id="TEST_PROGRAM"
                    )
                    
                    if stream_url:
                        successful_streams += 1
                        recovery_time = time.time() - failure_start
                        recovery_metrics['recovery_time_seconds'].append(recovery_time)
                        recovery_metrics['successful_recoveries'] += 1
                        recovery_metrics['successful_retries'] += 1
                        print(f"   âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¥ç¶šæˆåŠŸ (è©¦è¡Œ{attempt + 1}å›ç›®)")
                        
                except Exception as e:
                    recovery_metrics['failed_requests'] += 1
                    print(f"   âŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¥ç¶šå¤±æ•— (è©¦è¡Œ{attempt + 1}å›ç›®): {e}")
                
                time.sleep(0.05)  # çŸ­ã„é–“éš”ã§ãƒªãƒˆãƒ©ã‚¤
            
            # ã‚·ãƒŠãƒªã‚ª4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¾©æ—§å¾Œã®çŠ¶æ…‹åŒæœŸ
            print("ğŸŒ ã‚·ãƒŠãƒªã‚ª4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¾©æ—§å¾Œã®çŠ¶æ…‹åŒæœŸãƒ†ã‚¹ãƒˆ")
            
            # çŠ¶æ…‹åŒæœŸãƒ†ã‚¹ãƒˆ
            with patch('src.scheduler.RecordingScheduler') as mock_scheduler_class:
                scheduler = Mock()
                scheduler.sync_with_server.return_value = True
                scheduler.get_pending_schedules.return_value = [
                    Mock(id="sync_1", status=ScheduleStatus.SCHEDULED),
                    Mock(id="sync_2", status=ScheduleStatus.SCHEDULED)
                ]
                mock_scheduler_class.return_value = scheduler
                
                # å¾©æ—§å¾Œã®åŒæœŸå‡¦ç†
                pending_schedules = scheduler.get_pending_schedules()
                sync_result = scheduler.sync_with_server()
                
                if sync_result and pending_schedules:
                    recovery_metrics['successful_recoveries'] += 1
                    print(f"   âœ… çŠ¶æ…‹åŒæœŸæˆåŠŸ ({len(pending_schedules)}å€‹ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«)")
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # å¾©æ—§æ€§èƒ½ã®æ¤œè¨¼
        assert recovery_metrics['successful_recoveries'] >= 4, \
            f"å¾©æ—§æˆåŠŸæ•°ä¸è¶³: {recovery_metrics['successful_recoveries']} (æœŸå¾…å€¤: â‰¥4)"
        
        assert recovery_metrics['network_failures_simulated'] >= 10, \
            f"éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆæ•°ä¸è¶³: {recovery_metrics['network_failures_simulated']} (æœŸå¾…å€¤: â‰¥10)"
        
        # å¾©æ—§æ™‚é–“ã®æ¤œè¨¼
        if recovery_metrics['recovery_time_seconds']:
            avg_recovery_time = sum(recovery_metrics['recovery_time_seconds']) / len(recovery_metrics['recovery_time_seconds'])
            assert avg_recovery_time < 5.0, f"å¹³å‡å¾©æ—§æ™‚é–“è¶…é: {avg_recovery_time:.2f}ç§’ (æœŸå¾…å€¤: <5.0ç§’)"
        
        # æˆåŠŸç‡ã®æ¤œè¨¼ï¼ˆæˆåŠŸã—ãŸãƒªãƒˆãƒ©ã‚¤ã«åŸºã¥ãï¼‰
        total_attempts = recovery_metrics['network_failures_simulated']
        successful_attempts = recovery_metrics['successful_retries']
        
        # å®Ÿéš›ã®å¾©æ—§ã‚·ãƒŠãƒªã‚ªæ•°ã§è¨ˆç®—ï¼ˆ4ã¤ã®ã‚·ãƒŠãƒªã‚ªï¼‰
        scenario_count = 4
        scenario_success_rate = recovery_metrics['successful_recoveries'] / scenario_count
        assert scenario_success_rate >= 0.75, f"ã‚·ãƒŠãƒªã‚ªå¾©æ—§æˆåŠŸç‡ä¸è¶³: {scenario_success_rate:.1%} (æœŸå¾…å€¤: â‰¥75%)"
        
        # å€‹åˆ¥ãƒªãƒˆãƒ©ã‚¤ã®æˆåŠŸç‡ã‚‚ç¢ºèª
        retry_success_rate = successful_attempts / max(total_attempts, 1) if total_attempts > 0 else 0
        print(f"   å€‹åˆ¥ãƒªãƒˆãƒ©ã‚¤æˆåŠŸç‡: {retry_success_rate:.1%}")
        
        print(f"âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆæ•°: {recovery_metrics['network_failures_simulated']}å›")
        print(f"   å¾©æ—§æˆåŠŸæ•°: {recovery_metrics['successful_recoveries']}å›")
        print(f"   å¹³å‡å¾©æ—§æ™‚é–“: {avg_recovery_time:.2f}ç§’" if recovery_metrics['recovery_time_seconds'] else "   å¹³å‡å¾©æ—§æ™‚é–“: N/A")
        print(f"   ã‚·ãƒŠãƒªã‚ªå¾©æ—§ç‡: {scenario_success_rate:.1%}")


@pytest.mark.e2e
@pytest.mark.failure_recovery
@pytest.mark.resource_intensive
class TestFailureRecoveryC2:
    """C2: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
    
    def test_storage_failure_recovery(self, temp_environment, test_config, 
                                    mock_external_services, resource_monitor):
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
        resource_monitor.start_monitoring(interval=0.5)
        start_time = time.time()
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        storage_metrics = {
            'disk_full_simulations': 0,
            'permission_errors': 0,
            'io_errors': 0,
            'successful_recoveries': 0,
            'backup_location_switches': 0,
            'cleanup_operations': 0,
            'space_freed_mb': 0,
            'recovered_files': 0
        }
        
        # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        primary_storage = temp_environment['data_dir']
        backup_storage = os.path.join(temp_environment['base_dir'], 'backup_storage')
        os.makedirs(backup_storage, exist_ok=True)
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with patch('src.file_manager.FileManager') as mock_file_manager_class, \
             patch('src.recording.RecordingManager') as mock_recording_class, \
             patch('builtins.open', create=True) as mock_open, \
             patch('os.makedirs') as mock_makedirs, \
             patch('shutil.disk_usage') as mock_disk_usage, \
             patch('shutil.move') as mock_move:
            
            # ãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®š
            file_manager = Mock()
            recording_manager = Mock()
            mock_file_manager_class.return_value = file_manager
            mock_recording_class.return_value = recording_manager
            
            # ã‚·ãƒŠãƒªã‚ª1: ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³
            print("ğŸ’¾ ã‚·ãƒŠãƒªã‚ª1: ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ (total, used, free)
            disk_usage_scenarios = [
                (1000000000, 950000000, 50000000),    # 95% ä½¿ç”¨ (å®¹é‡ä¸è¶³)
                (1000000000, 950000000, 50000000),    # ã¾ã å®¹é‡ä¸è¶³
                (1000000000, 700000000, 300000000),   # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ
            ]
            mock_disk_usage.side_effect = disk_usage_scenarios
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿å¤±æ•—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            write_side_effects = [
                OSError("No space left on device"),
                OSError("No space left on device"),
                MagicMock()  # æˆåŠŸ
            ]
            
            mock_open.side_effect = write_side_effects
            
            # å®¹é‡ä¸è¶³ã‹ã‚‰ã®å¾©æ—§ãƒ†ã‚¹ãƒˆ
            for attempt in range(3):
                try:
                    storage_metrics['disk_full_simulations'] += 1
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿è©¦è¡Œ
                    with open(os.path.join(primary_storage, f"test_recording_{attempt}.aac"), "wb") as f:
                        f.write(b"test_audio_data")
                    
                    storage_metrics['successful_recoveries'] += 1
                    print(f"   âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿æˆåŠŸ (è©¦è¡Œ{attempt + 1}å›ç›®)")
                    break
                    
                except OSError as e:
                    if "No space left on device" in str(e):
                        print(f"   âš ï¸ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³æ¤œå‡º (è©¦è¡Œ{attempt + 1}å›ç›®)")
                        
                        # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                        file_manager.cleanup_old_files.return_value = True
                        cleanup_result = file_manager.cleanup_old_files(
                            max_age_days=7,
                            target_free_space_mb=500
                        )
                        
                        if cleanup_result:
                            storage_metrics['cleanup_operations'] += 1
                            storage_metrics['space_freed_mb'] += 250  # 250MBè§£æ”¾
                            print(f"   ğŸ§¹ è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ (250MBè§£æ”¾)")
            
            # ã‚·ãƒŠãƒªã‚ª2: æ¨©é™ã‚¨ãƒ©ãƒ¼
            print("ğŸ’¾ ã‚·ãƒŠãƒªã‚ª2: æ¨©é™ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # æ¨©é™ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            permission_side_effects = [
                PermissionError("Permission denied"),
                PermissionError("Permission denied"),
                MagicMock()  # æ¨©é™ä¿®æ­£å¾ŒæˆåŠŸ
            ]
            
            mock_makedirs.side_effect = permission_side_effects
            
            for attempt in range(3):
                try:
                    storage_metrics['permission_errors'] += 1
                    
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆè©¦è¡Œ
                    os.makedirs(os.path.join(primary_storage, f"permission_test_{attempt}"), exist_ok=True)
                    
                    storage_metrics['successful_recoveries'] += 1
                    print(f"   âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆæˆåŠŸ (è©¦è¡Œ{attempt + 1}å›ç›®)")
                    break
                    
                except PermissionError as e:
                    print(f"   âš ï¸ æ¨©é™ã‚¨ãƒ©ãƒ¼æ¤œå‡º (è©¦è¡Œ{attempt + 1}å›ç›®)")
                    
                    # æ¨©é™ä¿®æ­£ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    if attempt < 2:  # æœ€å¾Œã®è©¦è¡Œå‰ã¾ã§
                        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€ã¸ã®åˆ‡ã‚Šæ›¿ãˆ
                        file_manager.switch_to_backup_location.return_value = backup_storage
                        backup_path = file_manager.switch_to_backup_location()
                        
                        if backup_path:
                            storage_metrics['backup_location_switches'] += 1
                            print(f"   ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€ã«åˆ‡ã‚Šæ›¿ãˆ: {backup_path}")
            
            # ã‚·ãƒŠãƒªã‚ª3: I/O ã‚¨ãƒ©ãƒ¼
            print("ğŸ’¾ ã‚·ãƒŠãƒªã‚ª3: I/O ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # I/O ã‚¨ãƒ©ãƒ¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            io_error_scenarios = [
                Exception("Input/output error"),
                Exception("Device not ready"),
                True  # å¾©æ—§æˆåŠŸ
            ]
            
            recording_manager.save_recording.side_effect = io_error_scenarios
            
            for attempt in range(3):
                try:
                    storage_metrics['io_errors'] += 1
                    
                    # éŒ²éŸ³ä¿å­˜è©¦è¡Œ
                    save_result = recording_manager.save_recording(
                        recording_data=b"test_recording_data",
                        output_path=os.path.join(primary_storage, f"io_test_{attempt}.aac")
                    )
                    
                    if save_result:
                        storage_metrics['successful_recoveries'] += 1
                        storage_metrics['recovered_files'] += 1
                        print(f"   âœ… éŒ²éŸ³ä¿å­˜æˆåŠŸ (è©¦è¡Œ{attempt + 1}å›ç›®)")
                        break
                        
                except Exception as e:
                    print(f"   âš ï¸ I/Oã‚¨ãƒ©ãƒ¼æ¤œå‡º (è©¦è¡Œ{attempt + 1}å›ç›®): {e}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    file_manager.check_filesystem_health.return_value = attempt >= 2  # 3å›ç›®ã§å›å¾©
                    health_check = file_manager.check_filesystem_health()
                    
                    if health_check:
                        print(f"   ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å›å¾©ç¢ºèª")
                    
                    time.sleep(0.1)
            
            # ã‚·ãƒŠãƒªã‚ª4: ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã®å¾©æ—§
            print("ğŸ’¾ ã‚·ãƒŠãƒªã‚ª4: ç ´æãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡ºã¨å¾©æ—§
            corrupted_files = [
                "corrupted_recording_1.aac",
                "corrupted_recording_2.aac", 
                "corrupted_recording_3.aac"
            ]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã†ã¡2ã¤ã‚’å¾©æ—§æˆåŠŸï¼‰
            file_manager.verify_file_integrity.return_value = False  # å¸¸ã«ç ´ææ¤œå‡º
            file_manager.restore_from_backup.return_value = True
            
            for i, file_name in enumerate(corrupted_files):
                print(f"   âš ï¸ ç ´æãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {file_name}")
                
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§
                restore_result = file_manager.restore_from_backup(file_name)
                
                if restore_result:
                    storage_metrics['recovered_files'] += 1
                    
                    # 2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¾©æ—§æˆåŠŸã€1ã¤ã¯å¤±æ•—
                    if i < 2:  # æœ€åˆã®2ã¤ã¯æˆåŠŸ
                        storage_metrics['successful_recoveries'] += 1
                        print(f"   âœ… ãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§æˆåŠŸ: {file_name}")
                    else:  # 3ã¤ç›®ã¯å¤±æ•—
                        print(f"   âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§å¤±æ•—: {file_name}")
            
            # ã‚·ãƒŠãƒªã‚ª5: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®šã®å‹•çš„å¤‰æ›´
            print("ğŸ’¾ ã‚·ãƒŠãƒªã‚ª5: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®šå‹•çš„å¤‰æ›´ãƒ†ã‚¹ãƒˆ")
            
            # è¨­å®šå¤‰æ›´ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            with patch('src.scheduler.RecordingScheduler') as mock_scheduler_class:
                scheduler = Mock()
                scheduler.update_storage_config.return_value = True
                scheduler.migrate_pending_recordings.return_value = 5  # 5ä»¶ç§»è¡Œ
                mock_scheduler_class.return_value = scheduler
                
                # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®šæ›´æ–°
                config_update_result = scheduler.update_storage_config({
                    'primary_storage': backup_storage,
                    'backup_enabled': True,
                    'auto_cleanup': True
                })
                
                if config_update_result:
                    # ä¿ç•™ä¸­éŒ²éŸ³ã®ç§»è¡Œ
                    migrated_count = scheduler.migrate_pending_recordings()
                    storage_metrics['recovered_files'] += migrated_count
                    storage_metrics['successful_recoveries'] += 1
                    print(f"   âœ… ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®šæ›´æ–°æˆåŠŸ ({migrated_count}ä»¶ç§»è¡Œ)")
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¾©æ—§æ€§èƒ½ã®æ¤œè¨¼
        total_failures = (storage_metrics['disk_full_simulations'] + 
                         storage_metrics['permission_errors'] + 
                         storage_metrics['io_errors'])
        
        assert storage_metrics['successful_recoveries'] >= 6, \
            f"å¾©æ—§æˆåŠŸæ•°ä¸è¶³: {storage_metrics['successful_recoveries']} (æœŸå¾…å€¤: â‰¥6)"
        
        assert total_failures >= 9, \
            f"éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆæ•°ä¸è¶³: {total_failures} (æœŸå¾…å€¤: â‰¥9)"
        
        # å¾©æ—§ç‡ã®æ¤œè¨¼
        recovery_rate = storage_metrics['successful_recoveries'] / max(total_failures, 1)
        assert recovery_rate >= 0.6, f"å¾©æ—§æˆåŠŸç‡ä¸è¶³: {recovery_rate:.1%} (æœŸå¾…å€¤: â‰¥60%)"
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ¤œè¨¼
        resource_summary = resource_monitor.get_summary()
        peak_memory_mb = resource_summary.get('peak_memory_mb', 0)
        assert peak_memory_mb < 512, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {peak_memory_mb:.1f}MB (æœŸå¾…å€¤: <512MB)"
        
        print(f"âœ… ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆæ•°: {total_failures}å›")
        print(f"   å¾©æ—§æˆåŠŸæ•°: {storage_metrics['successful_recoveries']}å›")
        print(f"   å¾©æ—§ç‡: {recovery_rate:.1%}")
        print(f"   å¾©æ—§ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {storage_metrics['recovered_files']}ä»¶")
        print(f"   ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ: {storage_metrics['cleanup_operations']}å›")
        print(f"   è§£æ”¾å®¹é‡: {storage_metrics['space_freed_mb']}MB")


@pytest.mark.e2e
@pytest.mark.failure_recovery
@pytest.mark.slow
class TestFailureRecoveryC3:
    """C3: ãƒ—ãƒ­ã‚»ã‚¹éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
    
    def test_process_failure_recovery(self, temp_environment, test_config, 
                                    mock_external_services, resource_monitor):
        """ãƒ—ãƒ­ã‚»ã‚¹éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
        resource_monitor.start_monitoring(interval=0.5)
        start_time = time.time()
        
        # ãƒ—ãƒ­ã‚»ã‚¹éšœå®³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        process_metrics = {
            'daemon_restarts': 0,
            'scheduler_recoveries': 0,
            'recording_recoveries': 0,
            'state_restorations': 0,
            'memory_leak_detections': 0,
            'graceful_shutdowns': 0,
            'emergency_stops': 0,
            'data_consistency_checks': 0
        }
        
        # ãƒ—ãƒ­ã‚»ã‚¹éšœå®³ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with patch('src.daemon.DaemonManager') as mock_daemon_class, \
             patch('src.scheduler.RecordingScheduler') as mock_scheduler_class, \
             patch('src.recording.RecordingManager') as mock_recording_class, \
             patch('os.kill') as mock_kill, \
             patch('psutil.Process') as mock_process_class:
            
            # ãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®š
            daemon = Mock()
            scheduler = Mock()
            recording_manager = Mock()
            
            mock_daemon_class.return_value = daemon
            mock_scheduler_class.return_value = scheduler
            mock_recording_class.return_value = recording_manager
            
            # ã‚·ãƒŠãƒªã‚ª1: ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ç•°å¸¸çµ‚äº†ã¨å¾©æ—§
            print("ğŸ”„ ã‚·ãƒŠãƒªã‚ª1: ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸çµ‚äº†å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            daemon_status_sequence = [
                DaemonStatus.RUNNING,
                DaemonStatus.ERROR,
                DaemonStatus.STARTING,
                DaemonStatus.RUNNING
            ]
            
            daemon.get_status.side_effect = daemon_status_sequence
            daemon.restart.return_value = True
            daemon.load_state_from_backup.return_value = True
            
            for i, expected_status in enumerate(daemon_status_sequence):
                current_status = daemon.get_status()
                
                if current_status == DaemonStatus.ERROR:
                    print(f"   âš ï¸ ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¨ãƒ©ãƒ¼æ¤œå‡º")
                    process_metrics['emergency_stops'] += 1
                    
                    # è‡ªå‹•å¾©æ—§å‡¦ç†
                    restart_result = daemon.restart()
                    if restart_result:
                        process_metrics['daemon_restarts'] += 1
                        
                        # çŠ¶æ…‹å¾©å…ƒ
                        state_restore = daemon.load_state_from_backup()
                        if state_restore:
                            process_metrics['state_restorations'] += 1
                            print(f"   âœ… ãƒ‡ãƒ¼ãƒ¢ãƒ³å¾©æ—§æˆåŠŸ (çŠ¶æ…‹å¾©å…ƒå«ã‚€)")
                
                elif current_status == DaemonStatus.RUNNING and i > 0:
                    print(f"   âœ… ãƒ‡ãƒ¼ãƒ¢ãƒ³æ­£å¸¸ç¨¼åƒç¢ºèª")
                
                time.sleep(0.1)
            
            # ã‚·ãƒŠãƒªã‚ª2: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å¾©æ—§
            print("ğŸ”„ ã‚·ãƒŠãƒªã‚ª2: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            scheduler_health_checks = [
                False,  # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯çŠ¶æ…‹
                False,  # ã¾ã ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯
                True,   # å¾©æ—§æˆåŠŸ
                True    # æ­£å¸¸ç¨¼åƒ
            ]
            
            scheduler.health_check.side_effect = scheduler_health_checks
            scheduler.force_restart.return_value = True
            scheduler.recover_pending_schedules.return_value = 8  # 8ä»¶å¾©æ—§
            
            deadlock_detected = False
            for i, is_healthy in enumerate(scheduler_health_checks):
                health_status = scheduler.health_check()
                
                if not health_status and not deadlock_detected:
                    print(f"   âš ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡º")
                    deadlock_detected = True
                    
                    # å¼·åˆ¶å†èµ·å‹•
                    restart_result = scheduler.force_restart()
                    if restart_result:
                        process_metrics['scheduler_recoveries'] += 1
                        
                        # ä¿ç•™ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å¾©æ—§
                        recovered_schedules = scheduler.recover_pending_schedules()
                        if recovered_schedules > 0:
                            print(f"   âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å¾©æ—§æˆåŠŸ ({recovered_schedules}ä»¶ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¾©æ—§)")
                
                elif health_status and deadlock_detected:
                    print(f"   âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ­£å¸¸ç¨¼åƒå¾©å¸°")
                    break
                
                time.sleep(0.1)
            
            # ã‚·ãƒŠãƒªã‚ª3: éŒ²éŸ³ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒãƒ³ã‚°å¾©æ—§
            print("ğŸ”„ ã‚·ãƒŠãƒªã‚ª3: éŒ²éŸ³ãƒ—ãƒ­ã‚»ã‚¹ãƒãƒ³ã‚°å¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # éŒ²éŸ³ãƒ—ãƒ­ã‚»ã‚¹ã®çŠ¶æ…‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            recording_states = [
                RecordingStatus.RECORDING,
                RecordingStatus.RECORDING,  # ãƒãƒ³ã‚°çŠ¶æ…‹
                RecordingStatus.RECORDING,  # ã¾ã ãƒãƒ³ã‚°
                RecordingStatus.CANCELLED,  # å¼·åˆ¶åœæ­¢
                RecordingStatus.RECORDING   # æ–°è¦éŒ²éŸ³é–‹å§‹
            ]
            
            mock_process = Mock()
            mock_process.is_running.side_effect = [True, True, True, False, True]
            mock_process.memory_info.return_value = Mock(rss=100*1024*1024)  # 100MB
            mock_process_class.return_value = mock_process
            
            recording_manager.get_active_recordings.return_value = [
                Mock(id="rec_1", status=RecordingStatus.RECORDING, start_time=time.time() - 3600),
                Mock(id="rec_2", status=RecordingStatus.RECORDING, start_time=time.time() - 1800)
            ]
            recording_manager.force_stop_recording.return_value = True
            recording_manager.restart_recording.return_value = True
            
            # ãƒãƒ³ã‚°æ¤œå‡ºã¨å¾©æ—§
            for i, status in enumerate(recording_states):
                active_recordings = recording_manager.get_active_recordings()
                
                for recording in active_recordings:
                    # é•·æ™‚é–“éŒ²éŸ³ãƒãƒ³ã‚°æ¤œå‡ºï¼ˆ1æ™‚é–“ä»¥ä¸Šï¼‰
                    recording_duration = time.time() - recording.start_time
                    
                    if recording_duration > 3600 and status == RecordingStatus.RECORDING:
                        print(f"   âš ï¸ éŒ²éŸ³ãƒ—ãƒ­ã‚»ã‚¹ãƒãƒ³ã‚°æ¤œå‡º: {recording.id}")
                        
                        # å¼·åˆ¶åœæ­¢
                        stop_result = recording_manager.force_stop_recording(recording.id)
                        if stop_result:
                            process_metrics['emergency_stops'] += 1
                            
                            # éŒ²éŸ³å†é–‹
                            restart_result = recording_manager.restart_recording(recording.id)
                            if restart_result:
                                process_metrics['recording_recoveries'] += 1
                                print(f"   âœ… éŒ²éŸ³ãƒ—ãƒ­ã‚»ã‚¹å¾©æ—§æˆåŠŸ: {recording.id}")
                
                time.sleep(0.1)
            
            # ã‚·ãƒŠãƒªã‚ª4: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºã¨å¯¾å‡¦
            print("ğŸ”„ ã‚·ãƒŠãƒªã‚ª4: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºå¯¾å‡¦ãƒ†ã‚¹ãƒˆ")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¢—åŠ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            memory_usage_mb = [50, 100, 200, 400, 800, 200, 100]  # MB
            memory_threshold_mb = 500
            
            daemon.get_memory_usage.side_effect = [mb * 1024 * 1024 for mb in memory_usage_mb]
            daemon.perform_garbage_collection.return_value = True
            daemon.restart_high_memory_processes.return_value = True
            
            for i, memory_mb in enumerate(memory_usage_mb):
                current_memory = daemon.get_memory_usage()
                current_memory_mb = current_memory / 1024 / 1024
                
                if current_memory_mb > memory_threshold_mb:
                    print(f"   âš ï¸ ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {current_memory_mb:.1f}MB")
                    process_metrics['memory_leak_detections'] += 1
                    
                    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
                    gc_result = daemon.perform_garbage_collection()
                    if gc_result:
                        print(f"   ğŸ§¹ ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ")
                    
                    # é«˜ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ã‚»ã‚¹ã®å†èµ·å‹•
                    if current_memory_mb > memory_threshold_mb * 1.5:  # 750MBè¶…é
                        restart_result = daemon.restart_high_memory_processes()
                        if restart_result:
                            process_metrics['scheduler_recoveries'] += 1
                            print(f"   ğŸ”„ é«˜ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•")
                
                time.sleep(0.1)
            
            # ã‚·ãƒŠãƒªã‚ª5: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç ´æã‹ã‚‰ã®å¾©æ—§
            print("ğŸ”„ ã‚·ãƒŠãƒªã‚ª5: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç ´æå¾©æ—§ãƒ†ã‚¹ãƒˆ")
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            with patch('builtins.open', create=True) as mock_open, \
                 patch('json.load') as mock_json_load, \
                 patch('shutil.copy2') as mock_copy:
                
                # ç ´æè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                mock_json_load.side_effect = [
                    json.JSONDecodeError("Invalid JSON", "", 0),
                    {"area_id": "JP13", "backup": True}  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§
                ]
                
                daemon.load_config.side_effect = [
                    Exception("Config file corrupted"),
                    True  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©æ—§æˆåŠŸ
                ]
                daemon.restore_config_from_backup.return_value = True
                
                try:
                    config_result = daemon.load_config()
                except Exception as e:
                    print(f"   âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç ´ææ¤œå‡º: {e}")
                    
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§
                    restore_result = daemon.restore_config_from_backup()
                    if restore_result:
                        process_metrics['state_restorations'] += 1
                        
                        # å¾©æ—§å¾Œã®è¨­å®šèª­ã¿è¾¼ã¿
                        retry_result = daemon.load_config()
                        if retry_result:
                            print(f"   âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§æˆåŠŸ")
            
            # ã‚·ãƒŠãƒªã‚ª6: ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ†ã‚¹ãƒˆ
            print("ğŸ”„ ã‚·ãƒŠãƒªã‚ª6: ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ†ã‚¹ãƒˆ")
            
            # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            daemon.save_current_state.return_value = True
            scheduler.stop_all_schedules.return_value = True
            recording_manager.stop_all_recordings.return_value = True
            daemon.graceful_shutdown.return_value = True
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            daemon.verify_data_consistency.return_value = True
            consistency_check = daemon.verify_data_consistency()
            if consistency_check:
                process_metrics['data_consistency_checks'] += 1
                print(f"   âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª")
            
            # çŠ¶æ…‹ä¿å­˜
            state_save = daemon.save_current_state()
            if state_save:
                print(f"   ğŸ’¾ ç¾åœ¨çŠ¶æ…‹ä¿å­˜å®Œäº†")
            
            # å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åœæ­¢
            schedule_stop = scheduler.stop_all_schedules()
            if schedule_stop:
                print(f"   â¸ï¸ å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åœæ­¢å®Œäº†")
            
            # å…¨éŒ²éŸ³åœæ­¢
            recording_stop = recording_manager.stop_all_recordings()
            if recording_stop:
                print(f"   â¹ï¸ å…¨éŒ²éŸ³åœæ­¢å®Œäº†")
            
            # ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Ÿè¡Œ
            shutdown_result = daemon.graceful_shutdown()
            if shutdown_result:
                process_metrics['graceful_shutdowns'] += 1
                print(f"   âœ… ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æˆåŠŸ")
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # ãƒ—ãƒ­ã‚»ã‚¹å¾©æ—§æ€§èƒ½ã®æ¤œè¨¼
        total_recoveries = (process_metrics['daemon_restarts'] + 
                           process_metrics['scheduler_recoveries'] + 
                           process_metrics['recording_recoveries'])
        
        assert total_recoveries >= 3, \
            f"å¾©æ—§æ“ä½œæ•°ä¸è¶³: {total_recoveries} (æœŸå¾…å€¤: â‰¥3)"
        
        assert process_metrics['state_restorations'] >= 2, \
            f"çŠ¶æ…‹å¾©å…ƒæ•°ä¸è¶³: {process_metrics['state_restorations']} (æœŸå¾…å€¤: â‰¥2)"
        
        assert process_metrics['graceful_shutdowns'] >= 1, \
            f"ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Ÿè¡Œãªã—: {process_metrics['graceful_shutdowns']} (æœŸå¾…å€¤: â‰¥1)"
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºã®æ¤œè¨¼
        assert process_metrics['memory_leak_detections'] >= 1, \
            f"ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºãªã—: {process_metrics['memory_leak_detections']} (æœŸå¾…å€¤: â‰¥1)"
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã®æ¤œè¨¼
        assert process_metrics['data_consistency_checks'] >= 1, \
            f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯æœªå®Ÿè¡Œ: {process_metrics['data_consistency_checks']} (æœŸå¾…å€¤: â‰¥1)"
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ¤œè¨¼
        resource_summary = resource_monitor.get_summary()
        peak_memory_mb = resource_summary.get('peak_memory_mb', 0)
        assert peak_memory_mb < 256, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {peak_memory_mb:.1f}MB (æœŸå¾…å€¤: <256MB)"
        
        print(f"âœ… ãƒ—ãƒ­ã‚»ã‚¹éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   ãƒ‡ãƒ¼ãƒ¢ãƒ³å†èµ·å‹•: {process_metrics['daemon_restarts']}å›")
        print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å¾©æ—§: {process_metrics['scheduler_recoveries']}å›")
        print(f"   éŒ²éŸ³ãƒ—ãƒ­ã‚»ã‚¹å¾©æ—§: {process_metrics['recording_recoveries']}å›")
        print(f"   çŠ¶æ…‹å¾©å…ƒ: {process_metrics['state_restorations']}å›")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {process_metrics['memory_leak_detections']}å›")
        print(f"   ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³: {process_metrics['graceful_shutdowns']}å›")
        print(f"   ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {process_metrics['data_consistency_checks']}å›")