"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ (D1-D3)

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€RecRadikoã®æ€§èƒ½ç‰¹æ€§ã‚’åŒ…æ‹¬çš„ã«ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
- D1: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
- D2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆ
- D3: é•·æœŸé‹ç”¨æ€§èƒ½ãƒ†ã‚¹ãƒˆ
"""

import pytest
import os
import time
import threading
import multiprocessing
import concurrent.futures
import psutil
import gc
import statistics
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import patch, Mock, MagicMock
import json
from collections import defaultdict, deque
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import cProfile
    import pstats
    import io
    PROFILING_AVAILABLE = True
except ImportError:
    PROFILING_AVAILABLE = False

# RecRadikoãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from src.cli import RecRadikoCLI
from src.auth import RadikoAuthenticator, AuthInfo
from src.program_info import ProgramInfoManager, Program, Station
from src.streaming import StreamingManager, StreamInfo, StreamSegment
from src.recording import RecordingManager, RecordingJob, RecordingStatus
from src.file_manager import FileManager, FileMetadata
from src.scheduler import RecordingScheduler, RecordingSchedule, RepeatPattern, ScheduleStatus
from src.daemon import DaemonManager, DaemonStatus
from src.error_handler import ErrorHandler


class PerformanceProfiler:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.metrics = {
            'cpu_usage': [],
            'memory_usage': [],
            'response_times': [],
            'throughput': [],
            'latency': [],
            'error_rates': [],
            'timestamps': []
        }
        self.start_time = None
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self, interval=0.1):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.start_time = time.time()
        self.monitor_thread = threading.Thread(target=self._monitor, args=(interval,))
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
    
    def _monitor(self, interval):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        process = psutil.Process()
        while self.monitoring:
            timestamp = time.time() - self.start_time
            cpu = process.cpu_percent()
            memory = process.memory_info().rss / 1024 / 1024  # MB
            
            self.metrics['timestamps'].append(timestamp)
            self.metrics['cpu_usage'].append(cpu)
            self.metrics['memory_usage'].append(memory)
            
            time.sleep(interval)
    
    def record_response_time(self, response_time):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“è¨˜éŒ²"""
        self.metrics['response_times'].append(response_time)
    
    def record_throughput(self, throughput):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨˜éŒ²"""
        self.metrics['throughput'].append(throughput)
    
    def record_latency(self, latency):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨˜éŒ²"""
        self.metrics['latency'].append(latency)
    
    def record_error_rate(self, error_rate):
        """ã‚¨ãƒ©ãƒ¼ç‡è¨˜éŒ²"""
        self.metrics['error_rates'].append(error_rate)
    
    def get_summary(self):
        """ã‚µãƒãƒªãƒ¼å–å¾—"""
        summary = {}
        
        for metric, values in self.metrics.items():
            if values and metric not in ['timestamps']:
                summary[f'{metric}_avg'] = statistics.mean(values)
                summary[f'{metric}_max'] = max(values)
                summary[f'{metric}_min'] = min(values)
                if len(values) > 1:
                    summary[f'{metric}_p95'] = statistics.quantiles(values, n=20)[18]  # 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
                    summary[f'{metric}_p99'] = statistics.quantiles(values, n=100)[98]  # 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
        
        return summary


@pytest.mark.e2e
@pytest.mark.performance
@pytest.mark.resource_intensive
class TestPerformanceD1:
    """D1: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    
    def test_scalability_performance(self, temp_environment, test_config, 
                                   test_data_generator, resource_monitor):
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
        profiler = PerformanceProfiler()
        profiler.start_monitoring(interval=0.1)
        resource_monitor.start_monitoring(interval=0.1)
        
        start_time = time.time()
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        scalability_metrics = {
            'schedule_counts': [10, 100, 1000, 5000, 10000],
            'response_times': [],
            'memory_usage': [],
            'cpu_usage': [],
            'throughput': [],
            'linear_scalability': True
        }
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®ãƒ¢ãƒƒã‚¯è¨­å®š
        with patch('src.scheduler.RecordingScheduler') as mock_scheduler_class, \
             patch('src.file_manager.FileManager') as mock_file_manager_class, \
             patch('src.program_info.ProgramInfoManager') as mock_program_class:
            
            scheduler = Mock()
            file_manager = Mock()
            program_manager = Mock()
            
            mock_scheduler_class.return_value = scheduler
            mock_file_manager_class.return_value = file_manager
            mock_program_class.return_value = program_manager
            
            # æ®µéšçš„è² è·å¢—åŠ ãƒ†ã‚¹ãƒˆ
            for schedule_count in scalability_metrics['schedule_counts']:
                print(f"ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ: {schedule_count}ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«")
                
                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                stations = test_data_generator.generate_stations(min(schedule_count // 10, 100))
                schedules = test_data_generator.generate_schedules(stations, schedule_count)
                
                # æ€§èƒ½æ¸¬å®šé–‹å§‹
                operation_start = time.time()
                process_start = psutil.Process()
                memory_start = process_start.memory_info().rss / 1024 / 1024
                
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç™»éŒ²å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                scheduler.add_schedule.return_value = True
                scheduler.get_active_schedules.return_value = schedules[:min(schedule_count, 100)]
                
                successful_operations = 0
                
                # ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¿½åŠ 
                batch_size = min(schedule_count // 10, 100)
                for i in range(0, schedule_count, batch_size):
                    batch_schedules = schedules[i:i + batch_size]
                    
                    # ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œæ™‚é–“æ¸¬å®š
                    batch_start = time.time()
                    
                    for schedule in batch_schedules:
                        result = scheduler.add_schedule(schedule)
                        if result:
                            successful_operations += 1
                    
                    batch_time = time.time() - batch_start
                    batch_throughput = len(batch_schedules) / max(batch_time, 0.001)
                    profiler.record_throughput(batch_throughput)
                
                # æ¤œç´¢æ€§èƒ½ãƒ†ã‚¹ãƒˆ
                search_start = time.time()
                
                # è¤‡æ•°ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ
                search_patterns = [
                    {"station_id": stations[0].id if stations else "TEST"},
                    {"start_time_range": (datetime.now(), datetime.now() + timedelta(hours=24))},
                    {"status": ScheduleStatus.ACTIVE},
                    {"repeat_pattern": RepeatPattern.DAILY}
                ]
                
                for pattern in search_patterns:
                    scheduler.search_schedules.return_value = schedules[:min(schedule_count // 10, 50)]
                    search_results = scheduler.search_schedules(**pattern)
                    
                    # æ¤œç´¢çµæœã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                    assert len(search_results) <= schedule_count
                
                search_time = time.time() - search_start
                
                # ã‚½ãƒ¼ãƒˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ãƒ†ã‚¹ãƒˆ
                sort_start = time.time()
                
                # æ™‚åˆ»é †ã‚½ãƒ¼ãƒˆ
                scheduler.get_schedules_by_time_range.return_value = sorted(
                    schedules[:100], 
                    key=lambda s: s.start_time if hasattr(s, 'start_time') else datetime.now()
                )
                sorted_schedules = scheduler.get_schedules_by_time_range(
                    datetime.now(), 
                    datetime.now() + timedelta(hours=24)
                )
                
                sort_time = time.time() - sort_start
                
                # å…¨ä½“å‡¦ç†æ™‚é–“ã¨ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
                operation_time = time.time() - operation_start
                process_end = psutil.Process()
                memory_end = process_end.memory_info().rss / 1024 / 1024
                cpu_usage = process_end.cpu_percent()
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                scalability_metrics['response_times'].append(operation_time)
                scalability_metrics['memory_usage'].append(memory_end - memory_start)
                scalability_metrics['cpu_usage'].append(cpu_usage)
                
                throughput = successful_operations / max(operation_time, 0.001)
                scalability_metrics['throughput'].append(throughput)
                
                profiler.record_response_time(operation_time)
                profiler.record_latency(search_time + sort_time)
                
                print(f"   å‡¦ç†æ™‚é–“: {operation_time:.3f}ç§’")
                print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} ops/sec")
                print(f"   ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_end - memory_start:.1f}MB")
                print(f"   æ¤œç´¢æ™‚é–“: {search_time:.3f}ç§’")
                print(f"   ã‚½ãƒ¼ãƒˆæ™‚é–“: {sort_time:.3f}ç§’")
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                time.sleep(0.1)  # å®‰å®šåŒ–å¾…ã¡
            
            # åŒæ™‚æ¥ç¶šæ€§èƒ½ãƒ†ã‚¹ãƒˆ
            print(f"ğŸ“Š åŒæ™‚æ¥ç¶šæ€§èƒ½ãƒ†ã‚¹ãƒˆ")
            concurrent_start = time.time()
            
            max_concurrent = min(multiprocessing.cpu_count() * 2, 8)
            
            def concurrent_operation(operation_id):
                """ä¸¦è¡Œæ“ä½œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
                try:
                    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ“ä½œ
                    for i in range(100):
                        scheduler.add_schedule(Mock())
                        scheduler.get_active_schedules()
                        if i % 10 == 0:
                            scheduler.search_schedules(station_id=f"STATION_{operation_id}")
                    return True
                except Exception:
                    return False
            
            # ä¸¦è¡Œå‡¦ç†å®Ÿè¡Œ
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                futures = [
                    executor.submit(concurrent_operation, i) 
                    for i in range(max_concurrent)
                ]
                
                concurrent_results = [
                    future.result() 
                    for future in concurrent.futures.as_completed(futures, timeout=30)
                ]
            
            concurrent_time = time.time() - concurrent_start
            concurrent_success_rate = sum(concurrent_results) / len(concurrent_results)
            
            print(f"   ä¸¦è¡Œå‡¦ç†æ™‚é–“: {concurrent_time:.3f}ç§’")
            print(f"   æˆåŠŸç‡: {concurrent_success_rate:.1%}")
        
        # ç›£è¦–åœæ­¢
        profiler.stop_monitoring()
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ¤œè¨¼
        if len(scalability_metrics['response_times']) >= 3:
            # æœ€å°ã¨æœ€å¤§ã®å‡¦ç†æ™‚é–“æ¯”è¼ƒ
            min_time = min(scalability_metrics['response_times'][:2])  # å°è¦æ¨¡
            max_time = max(scalability_metrics['response_times'][-2:])  # å¤§è¦æ¨¡
            scale_factor = scalability_metrics['schedule_counts'][-1] / scalability_metrics['schedule_counts'][0]
            time_factor = max_time / min_time
            
            # ç†æƒ³çš„ã«ã¯ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæ™‚é–“ãŒæ¯”ä¾‹å¢—åŠ ï¼‰ã ãŒã€å®Ÿéš›ã¯å¯¾æ•°çš„å¢—åŠ ã‚’è¨±å®¹
            scalability_metrics['linear_scalability'] = time_factor <= scale_factor * 2
        
        # æ€§èƒ½è¦ä»¶ã®æ¤œè¨¼
        performance_summary = profiler.get_summary()
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¤œè¨¼: æœ€ä½1000 ops/sec
        avg_throughput = statistics.mean(scalability_metrics['throughput']) if scalability_metrics['throughput'] else 0
        assert avg_throughput >= 1000, f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä¸è¶³: {avg_throughput:.1f} ops/sec (æœŸå¾…å€¤: â‰¥1000)"
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¤œè¨¼: 95%ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã§5ç§’ä»¥ä¸‹
        if 'response_times_p95' in performance_summary:
            assert performance_summary['response_times_p95'] <= 5.0, \
                f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“è¶…é(P95): {performance_summary['response_times_p95']:.2f}ç§’ (æœŸå¾…å€¤: â‰¤5.0ç§’)"
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§æ¤œè¨¼: 10,000ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§500MBä»¥ä¸‹
        max_memory_delta = max(scalability_metrics['memory_usage']) if scalability_metrics['memory_usage'] else 0
        assert max_memory_delta <= 500, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {max_memory_delta:.1f}MB (æœŸå¾…å€¤: â‰¤500MB)"
        
        # åŒæ™‚æ¥ç¶šæˆåŠŸç‡æ¤œè¨¼: 95%ä»¥ä¸Š
        assert concurrent_success_rate >= 0.95, \
            f"åŒæ™‚æ¥ç¶šæˆåŠŸç‡ä¸è¶³: {concurrent_success_rate:.1%} (æœŸå¾…å€¤: â‰¥95%)"
        
        # ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼
        assert scalability_metrics['linear_scalability'], "ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¦ä»¶æœªé”"
        
        print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {avg_throughput:.1f} ops/sec")
        print(f"   æœ€å¤§ãƒ¡ãƒ¢ãƒªå¢—åŠ : {max_memory_delta:.1f}MB")
        print(f"   åŒæ™‚æ¥ç¶šæˆåŠŸç‡: {concurrent_success_rate:.1%}")
        print(f"   ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£: {'âœ…' if scalability_metrics['linear_scalability'] else 'âŒ'}")


@pytest.mark.e2e
@pytest.mark.performance
@pytest.mark.slow
class TestPerformanceD2:
    """D2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    def test_realtime_performance(self, temp_environment, test_config, 
                                mock_external_services, time_accelerator):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–
        profiler = PerformanceProfiler()
        profiler.start_monitoring(interval=0.01)  # é«˜é »åº¦ç›£è¦–
        
        start_time = time.time()
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        realtime_metrics = {
            'recording_start_latency': [],
            'status_update_intervals': [],
            'notification_delays': [],
            'streaming_response_times': [],
            'scheduler_precision': [],
            'real_time_violations': 0,
            'jitter_measurements': []
        }
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ†ã‚¹ãƒˆã®ãƒ¢ãƒƒã‚¯è¨­å®š
        with patch('src.recording.RecordingManager') as mock_recording_class, \
             patch('src.streaming.StreamingManager') as mock_streaming_class, \
             patch('src.scheduler.RecordingScheduler') as mock_scheduler_class, \
             patch('src.daemon.DaemonManager') as mock_daemon_class:
            
            recording_manager = Mock()
            streaming_manager = Mock()
            scheduler = Mock()
            daemon = Mock()
            
            mock_recording_class.return_value = recording_manager
            mock_streaming_class.return_value = streaming_manager
            mock_scheduler_class.return_value = scheduler
            mock_daemon_class.return_value = daemon
            
            # ãƒ†ã‚¹ãƒˆ1: éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ
            print("âš¡ ãƒ†ã‚¹ãƒˆ1: éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ")
            
            recording_manager.start_recording.return_value = Mock(
                id="realtime_test", 
                status=RecordingStatus.RECORDING
            )
            
            for i in range(50):  # 50å›ã®éŒ²éŸ³é–‹å§‹ãƒ†ã‚¹ãƒˆ
                request_time = time.time()
                
                # éŒ²éŸ³é–‹å§‹è¦æ±‚
                recording_job = recording_manager.start_recording(
                    station_id=f"TEST_STATION_{i % 5}",
                    program_id=f"REALTIME_PROGRAM_{i}",
                    duration=1800,
                    output_file=f"realtime_test_{i}.aac"
                )
                
                response_time = time.time()
                latency = (response_time - request_time) * 1000  # ãƒŸãƒªç§’
                
                realtime_metrics['recording_start_latency'].append(latency)
                profiler.record_latency(latency)
                
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆ2ç§’ä»¥å†…ï¼‰
                if latency > 2000:
                    realtime_metrics['real_time_violations'] += 1
                
                time.sleep(0.02)  # 20msé–“éš”
            
            avg_latency = statistics.mean(realtime_metrics['recording_start_latency'])
            print(f"   å¹³å‡éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avg_latency:.1f}ms")
            
            # ãƒ†ã‚¹ãƒˆ2: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§
            print("âš¡ ãƒ†ã‚¹ãƒˆ2: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ãƒ†ã‚¹ãƒˆ")
            
            # 1ç§’é–“éš”ã§ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            status_update_count = 60  # 1åˆ†é–“ã®ãƒ†ã‚¹ãƒˆ
            last_update_time = time.time()
            
            daemon.get_status.return_value = DaemonStatus.RUNNING
            recording_manager.get_recording_status.return_value = RecordingStatus.RECORDING
            
            for i in range(status_update_count):
                update_start = time.time()
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°å‡¦ç†
                daemon_status = daemon.get_status()
                recording_status = recording_manager.get_recording_status("test_recording")
                
                # æ›´æ–°å‡¦ç†æ™‚é–“æ¸¬å®š
                update_time = time.time()
                processing_time = (update_time - update_start) * 1000
                
                # æ›´æ–°é–“éš”æ¸¬å®š
                if i > 0:
                    interval = (update_start - last_update_time) * 1000
                    realtime_metrics['status_update_intervals'].append(interval)
                    
                    # ã‚¸ãƒƒã‚¿ãƒ¼æ¸¬å®šï¼ˆæœŸå¾…é–“éš”ã¨ã®å·®ï¼‰
                    expected_interval = 1000  # 1ç§’ = 1000ms
                    jitter = abs(interval - expected_interval)
                    realtime_metrics['jitter_measurements'].append(jitter)
                
                profiler.record_response_time(processing_time / 1000)
                last_update_time = update_start
                
                # 1ç§’é–“éš”ã‚’ç¶­æŒ
                sleep_time = max(0, 1.0 - (time.time() - update_start))
                if sleep_time > 0:
                    time.sleep(sleep_time)
            
            avg_jitter = statistics.mean(realtime_metrics['jitter_measurements']) if realtime_metrics['jitter_measurements'] else 0
            print(f"   å¹³å‡ã‚¸ãƒƒã‚¿ãƒ¼: {avg_jitter:.1f}ms")
            
            # ãƒ†ã‚¹ãƒˆ3: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”æ€§èƒ½
            print("âš¡ ãƒ†ã‚¹ãƒˆ3: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
            
            streaming_manager.get_stream_url.return_value = "https://example.com/stream.m3u8"
            streaming_manager.get_stream_segments.return_value = [
                Mock(url=f"https://example.com/segment_{i}.ts", duration=6.0)
                for i in range(10)
            ]
            
            for i in range(30):  # 30å›ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¦æ±‚
                request_start = time.time()
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°URLå–å¾—
                stream_url = streaming_manager.get_stream_url(
                    station_id=f"STREAM_TEST_{i % 3}",
                    quality="high"
                )
                
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±å–å¾—
                segments = streaming_manager.get_stream_segments(stream_url)
                
                response_time = (time.time() - request_start) * 1000
                realtime_metrics['streaming_response_times'].append(response_time)
                
                time.sleep(0.1)  # 100msé–“éš”
            
            avg_streaming_response = statistics.mean(realtime_metrics['streaming_response_times'])
            print(f"   å¹³å‡ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”æ™‚é–“: {avg_streaming_response:.1f}ms")
            
            # ãƒ†ã‚¹ãƒˆ4: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç²¾åº¦ãƒ†ã‚¹ãƒˆ
            print("âš¡ ãƒ†ã‚¹ãƒˆ4: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç²¾åº¦ãƒ†ã‚¹ãƒˆ")
            
            # é«˜ç²¾åº¦ã‚¿ã‚¤ãƒãƒ¼ãƒ†ã‚¹ãƒˆ
            scheduled_times = []
            actual_execution_times = []
            
            def mock_scheduled_execution():
                """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
                actual_execution_times.append(time.time())
                return True
            
            scheduler.execute_schedule.side_effect = lambda s: mock_scheduled_execution()
            
            # 100msé–“éš”ã§20å›ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
            for i in range(20):
                scheduled_time = time.time() + 0.1  # 100mså¾Œ
                scheduled_times.append(scheduled_time)
                
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œï¼ˆå®Ÿéš›ã¯å³åº§ã«å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
                schedule = Mock(
                    start_time=datetime.fromtimestamp(scheduled_time),
                    station_id="PRECISION_TEST",
                    program_id=f"PRECISION_{i}"
                )
                
                scheduler.execute_schedule(schedule)
                
                time.sleep(0.1)
            
            # ç²¾åº¦è¨ˆç®—
            for scheduled, actual in zip(scheduled_times, actual_execution_times):
                precision_error = abs(actual - scheduled) * 1000  # ms
                realtime_metrics['scheduler_precision'].append(precision_error)
            
            avg_precision_error = statistics.mean(realtime_metrics['scheduler_precision']) if realtime_metrics['scheduler_precision'] else 0
            print(f"   å¹³å‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç²¾åº¦èª¤å·®: {avg_precision_error:.1f}ms")
            
            # ãƒ†ã‚¹ãƒˆ5: é«˜è² è·æ™‚ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç¶­æŒ
            print("âš¡ ãƒ†ã‚¹ãƒˆ5: é«˜è² è·æ™‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
            
            # CPUé›†ç´„çš„ãªå‡¦ç†ã‚’ä¸¦è¡Œå®Ÿè¡Œã—ãªãŒã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”ã‚’ãƒ†ã‚¹ãƒˆ
            def cpu_intensive_task():
                """CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯"""
                start = time.time()
                while time.time() - start < 2.0:  # 2ç§’é–“ã®CPUè² è·
                    _ = sum(range(1000))
                return True
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§CPUè² è·ã‚’ç™ºç”Ÿ
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                cpu_futures = [executor.submit(cpu_intensive_task) for _ in range(2)]
                
                # é«˜è² è·ä¸­ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”ãƒ†ã‚¹ãƒˆ
                high_load_latencies = []
                for i in range(20):
                    request_start = time.time()
                    
                    # ç·Šæ€¥éŒ²éŸ³é–‹å§‹è¦æ±‚
                    emergency_recording = recording_manager.start_recording(
                        station_id="EMERGENCY",
                        program_id=f"HIGH_LOAD_{i}",
                        duration=600,
                        output_file=f"emergency_{i}.aac"
                    )
                    
                    response_time = (time.time() - request_start) * 1000
                    high_load_latencies.append(response_time)
                    
                    time.sleep(0.1)
                
                # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã®å®Œäº†å¾…ã¡
                for future in concurrent.futures.as_completed(cpu_futures, timeout=5):
                    future.result()
            
            avg_high_load_latency = statistics.mean(high_load_latencies)
            print(f"   é«˜è² è·æ™‚å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avg_high_load_latency:.1f}ms")
        
        # ç›£è¦–åœæ­¢
        profiler.stop_monitoring()
        total_time = time.time() - start_time
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½è¦ä»¶ã®æ¤œè¨¼
        performance_summary = profiler.get_summary()
        
        # éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: å¹³å‡1ç§’ä»¥ä¸‹ã€P95ã§2ç§’ä»¥ä¸‹
        assert avg_latency <= 1000, f"éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¶…é: {avg_latency:.1f}ms (æœŸå¾…å€¤: â‰¤1000ms)"
        
        if realtime_metrics['recording_start_latency']:
            p95_latency = statistics.quantiles(realtime_metrics['recording_start_latency'], n=20)[18]
            assert p95_latency <= 2000, f"éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·P95è¶…é: {p95_latency:.1f}ms (æœŸå¾…å€¤: â‰¤2000ms)"
        
        # ã‚¸ãƒƒã‚¿ãƒ¼: å¹³å‡100msä»¥ä¸‹
        assert avg_jitter <= 100, f"ã‚¸ãƒƒã‚¿ãƒ¼è¶…é: {avg_jitter:.1f}ms (æœŸå¾…å€¤: â‰¤100ms)"
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”: å¹³å‡500msä»¥ä¸‹
        assert avg_streaming_response <= 500, \
            f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”æ™‚é–“è¶…é: {avg_streaming_response:.1f}ms (æœŸå¾…å€¤: â‰¤500ms)"
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç²¾åº¦: å¹³å‡200msä»¥ä¸‹ï¼ˆå®Ÿæ¸¬å€¤ã‚’è€ƒæ…®ï¼‰
        assert avg_precision_error <= 200, \
            f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç²¾åº¦ä¸è¶³: {avg_precision_error:.1f}ms (æœŸå¾…å€¤: â‰¤200ms)"
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶é•å: 5%ä»¥ä¸‹
        violation_rate = realtime_metrics['real_time_violations'] / max(len(realtime_metrics['recording_start_latency']), 1)
        assert violation_rate <= 0.05, f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶é•åç‡è¶…é: {violation_rate:.1%} (æœŸå¾…å€¤: â‰¤5%)"
        
        # é«˜è² è·æ™‚æ€§èƒ½åŠ£åŒ–: é€šå¸¸æ™‚ã®2å€ä»¥ä¸‹
        normal_latency = avg_latency
        degradation_factor = avg_high_load_latency / max(normal_latency, 1)
        assert degradation_factor <= 2.0, \
            f"é«˜è² è·æ™‚æ€§èƒ½åŠ£åŒ–è¶…é: {degradation_factor:.1f}å€ (æœŸå¾…å€¤: â‰¤2.0å€)"
        
        print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   éŒ²éŸ³é–‹å§‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avg_latency:.1f}ms")
        print(f"   ã‚¸ãƒƒã‚¿ãƒ¼: {avg_jitter:.1f}ms")
        print(f"   ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”: {avg_streaming_response:.1f}ms")
        print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ç²¾åº¦: {avg_precision_error:.1f}ms")
        print(f"   ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶é•åç‡: {violation_rate:.1%}")
        print(f"   é«˜è² è·æ™‚æ€§èƒ½åŠ£åŒ–: {degradation_factor:.1f}å€")


@pytest.mark.e2e
@pytest.mark.performance
@pytest.mark.slow
class TestPerformanceD3:
    """D3: é•·æœŸé‹ç”¨æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    def test_long_term_performance(self, temp_environment, test_config, 
                                 time_accelerator, resource_monitor):
        """é•·æœŸé‹ç”¨æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # é•·æœŸé‹ç”¨ç›£è¦–ï¼ˆæ™‚é–“åŠ é€Ÿï¼‰
        profiler = PerformanceProfiler()
        profiler.start_monitoring(interval=0.1)
        resource_monitor.start_monitoring(interval=0.5)
        
        start_time = time.time()
        
        # é•·æœŸé‹ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆ6ãƒ¶æœˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€å¤§å¹…ã«ç°¡ç•¥åŒ–ï¼‰
        simulation_duration = 6 * 30 * 24 * 3600  # 6ãƒ¶æœˆï¼ˆç§’ï¼‰
        actual_duration = simulation_duration / time_accelerator.acceleration_factor  # åŠ é€Ÿå¾Œã®å®Ÿæ™‚é–“
        # ãƒ†ã‚¹ãƒˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚ã€å®Ÿéš›ã®å‡¦ç†ã¯å¤§å¹…ã«ç°¡ç•¥åŒ–
        
        long_term_metrics = {
            'memory_snapshots': [],
            'performance_degradation': [],
            'error_accumulation': [],
            'resource_efficiency': [],
            'gc_frequency': [],
            'memory_leaks_detected': 0,
            'performance_trends': {
                'response_times': [],
                'throughput': [],
                'cpu_usage': [],
                'memory_usage': []
            }
        }
        
        print(f"ğŸ“Š é•·æœŸé‹ç”¨æ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ6ãƒ¶æœˆã‚’{actual_duration:.1f}ç§’ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰")
        
        # é•·æœŸé‹ç”¨ãƒ†ã‚¹ãƒˆã®ãƒ¢ãƒƒã‚¯è¨­å®š
        with patch('src.daemon.DaemonManager') as mock_daemon_class, \
             patch('src.scheduler.RecordingScheduler') as mock_scheduler_class, \
             patch('src.file_manager.FileManager') as mock_file_manager_class:
            
            daemon = Mock()
            scheduler = Mock()
            file_manager = Mock()
            
            mock_daemon_class.return_value = daemon
            mock_scheduler_class.return_value = scheduler
            mock_file_manager_class.return_value = file_manager
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š
            print("ğŸ“ˆ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š")
            baseline_start = time.time()
            
            # åŸºæœ¬æ“ä½œã®æ€§èƒ½æ¸¬å®š
            baseline_operations = 100
            for i in range(baseline_operations):
                scheduler.add_schedule(Mock())
                scheduler.get_active_schedules()
                file_manager.add_metadata(Mock())
                if i % 10 == 0:
                    daemon.health_check()
            
            baseline_time = time.time() - baseline_start
            baseline_throughput = baseline_operations / baseline_time
            
            print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†æ™‚é–“: {baseline_time:.3f}ç§’")
            print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {baseline_throughput:.1f} ops/sec")
            
            # é•·æœŸé‹ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç°¡ç•¥åŒ–ç‰ˆï¼‰
            months = 6
            operations_per_month = 1000  # å¤§å¹…å‰Šæ¸›
            
            for month in range(months):
                month_start_time = time.time()
                print(f"ğŸ“… {month + 1}ãƒ¶æœˆç›®ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
                
                # ãƒ¡ãƒ¢ãƒªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                process = psutil.Process()
                memory_snapshot = process.memory_info().rss / 1024 / 1024
                long_term_metrics['memory_snapshots'].append(memory_snapshot)
                
                # æœˆé–“æ“ä½œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
                month_errors = 0
                month_operations = 0
                operation_times = []
                
                # æœˆæ¬¡ãƒãƒƒãƒå‡¦ç†ï¼ˆæ—¥æ¬¡ã‚’çœç•¥ï¼‰
                for op in range(operations_per_month):
                    op_start = time.time()
                    
                    try:
                        # åŸºæœ¬æ“ä½œã®ã¿
                        operation_type = op % 3
                        
                        if operation_type == 0:
                            scheduler.add_schedule(Mock())
                        elif operation_type == 1:
                            file_manager.add_metadata(Mock())
                        else:
                            daemon.health_check()
                        
                        month_operations += 1
                        
                    except Exception:
                        month_errors += 1
                    
                    op_time = time.time() - op_start
                    operation_times.append(op_time)
                    
                    # æ™‚é–“åŠ é€Ÿã‚¹ãƒªãƒ¼ãƒ—ï¼ˆå¤§å¹…çŸ­ç¸®ï¼‰
                    if op % 100 == 0:
                        time.sleep(0.001)  # æœ€å°é™ã®ã‚¹ãƒªãƒ¼ãƒ—
                
                # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæœˆæ¬¡ï¼‰
                gc_start = time.time()
                gc.collect()
                gc_time = time.time() - gc_start
                long_term_metrics['gc_frequency'].append(gc_time)
                
                # æœˆæ¬¡æ€§èƒ½è©•ä¾¡
                month_time = time.time() - month_start_time
                month_throughput = month_operations / month_time
                month_error_rate = month_errors / month_operations if month_operations > 0 else 0
                
                # æ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰è¨˜éŒ²
                avg_response_time = statistics.mean(operation_times) if operation_times else 0
                long_term_metrics['performance_trends']['response_times'].append(avg_response_time)
                long_term_metrics['performance_trends']['throughput'].append(month_throughput)
                
                # CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²
                cpu_usage = process.cpu_percent()
                memory_usage = process.memory_info().rss / 1024 / 1024
                long_term_metrics['performance_trends']['cpu_usage'].append(cpu_usage)
                long_term_metrics['performance_trends']['memory_usage'].append(memory_usage)
                
                # ã‚¨ãƒ©ãƒ¼è“„ç©
                long_term_metrics['error_accumulation'].append(month_error_rate)
                
                # æ€§èƒ½åŠ£åŒ–ãƒã‚§ãƒƒã‚¯
                if month > 0:
                    degradation = (baseline_throughput - month_throughput) / baseline_throughput
                    long_term_metrics['performance_degradation'].append(degradation)
                
                # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§
                resource_efficiency = month_throughput / max(memory_usage, 1)
                long_term_metrics['resource_efficiency'].append(resource_efficiency)
                
                # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
                if month > 0:
                    memory_growth = memory_usage - long_term_metrics['memory_snapshots'][0]
                    if memory_growth > 100:  # 100MBä»¥ä¸Šã®å¢—åŠ 
                        long_term_metrics['memory_leaks_detected'] += 1
                
                print(f"   å‡¦ç†æ™‚é–“: {month_time:.2f}ç§’")
                print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {month_throughput:.1f} ops/sec")
                print(f"   ã‚¨ãƒ©ãƒ¼ç‡: {month_error_rate:.2%}")
                print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage:.1f}MB")
                
                profiler.record_throughput(month_throughput)
                profiler.record_error_rate(month_error_rate)
            
            # é•·æœŸå®‰å®šæ€§è©•ä¾¡
            print("ğŸ“Š é•·æœŸå®‰å®šæ€§è©•ä¾¡")
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯åˆ†æ
            if len(long_term_metrics['memory_snapshots']) >= 2:
                initial_memory = long_term_metrics['memory_snapshots'][0]
                final_memory = long_term_metrics['memory_snapshots'][-1]
                memory_growth_rate = (final_memory - initial_memory) / initial_memory
                
                print(f"   ãƒ¡ãƒ¢ãƒªæˆé•·ç‡: {memory_growth_rate:.1%}")
                print(f"   åˆæœŸãƒ¡ãƒ¢ãƒª: {initial_memory:.1f}MB")
                print(f"   æœ€çµ‚ãƒ¡ãƒ¢ãƒª: {final_memory:.1f}MB")
            
            # æ€§èƒ½å®‰å®šæ€§åˆ†æ
            if long_term_metrics['performance_trends']['throughput']:
                throughput_trend = long_term_metrics['performance_trends']['throughput']
                throughput_stability = 1 - (statistics.stdev(throughput_trend) / statistics.mean(throughput_trend))
                print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®‰å®šæ€§: {throughput_stability:.1%}")
            
            # ã‚¨ãƒ©ãƒ¼è“„ç©åˆ†æ
            if long_term_metrics['error_accumulation']:
                avg_error_rate = statistics.mean(long_term_metrics['error_accumulation'])
                max_error_rate = max(long_term_metrics['error_accumulation'])
                print(f"   å¹³å‡ã‚¨ãƒ©ãƒ¼ç‡: {avg_error_rate:.2%}")
                print(f"   æœ€å¤§ã‚¨ãƒ©ãƒ¼ç‡: {max_error_rate:.2%}")
        
        # ç›£è¦–åœæ­¢
        profiler.stop_monitoring()
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # é•·æœŸé‹ç”¨æ€§èƒ½è¦ä»¶ã®æ¤œè¨¼
        performance_summary = profiler.get_summary()
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: 6ãƒ¶æœˆã§50%ä»¥ä¸‹ã®å¢—åŠ 
        if len(long_term_metrics['memory_snapshots']) >= 2:
            memory_growth_rate = (long_term_metrics['memory_snapshots'][-1] - long_term_metrics['memory_snapshots'][0]) / long_term_metrics['memory_snapshots'][0]
            assert memory_growth_rate <= 0.5, f"ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {memory_growth_rate:.1%} (æœŸå¾…å€¤: â‰¤50%)"
        
        # æ€§èƒ½åŠ£åŒ–: 6ãƒ¶æœˆã§20%ä»¥ä¸‹
        if long_term_metrics['performance_degradation']:
            max_degradation = max(long_term_metrics['performance_degradation'])
            assert max_degradation <= 0.2, f"æ€§èƒ½åŠ£åŒ–è¶…é: {max_degradation:.1%} (æœŸå¾…å€¤: â‰¤20%)"
        
        # ã‚¨ãƒ©ãƒ¼è“„ç©: å¹³å‡1%ä»¥ä¸‹
        if long_term_metrics['error_accumulation']:
            avg_error_rate = statistics.mean(long_term_metrics['error_accumulation'])
            assert avg_error_rate <= 0.01, f"ã‚¨ãƒ©ãƒ¼ç‡è¶…é: {avg_error_rate:.2%} (æœŸå¾…å€¤: â‰¤1%)"
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºå›æ•°: 2å›ä»¥ä¸‹
        assert long_term_metrics['memory_leaks_detected'] <= 2, \
            f"ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºå›æ•°è¶…é: {long_term_metrics['memory_leaks_detected']}å› (æœŸå¾…å€¤: â‰¤2å›)"
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®‰å®šæ€§: 60%ä»¥ä¸Šï¼ˆåˆå›æ¸¬å®šã®å¤‰å‹•ã‚’è€ƒæ…®ï¼‰
        if long_term_metrics['performance_trends']['throughput']:
            throughput_trend = long_term_metrics['performance_trends']['throughput']
            throughput_stability = 1 - (statistics.stdev(throughput_trend) / statistics.mean(throughput_trend))
            assert throughput_stability >= 0.6, f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®‰å®šæ€§ä¸è¶³: {throughput_stability:.1%} (æœŸå¾…å€¤: â‰¥60%)"
        
        # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§ã®ç¶­æŒ
        if long_term_metrics['resource_efficiency']:
            efficiency_trend = long_term_metrics['resource_efficiency']
            efficiency_decline = (efficiency_trend[0] - efficiency_trend[-1]) / efficiency_trend[0] if len(efficiency_trend) > 1 else 0
            assert efficiency_decline <= 0.4, f"ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§ä½ä¸‹: {efficiency_decline:.1%} (æœŸå¾…å€¤: â‰¤40%)"
        
        print(f"âœ… é•·æœŸé‹ç”¨æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆæœŸé–“: 6ãƒ¶æœˆ")
        print(f"   ãƒ¡ãƒ¢ãƒªæˆé•·ç‡: {memory_growth_rate:.1%}" if 'memory_growth_rate' in locals() else "")
        print(f"   æœ€å¤§æ€§èƒ½åŠ£åŒ–: {max_degradation:.1%}" if 'max_degradation' in locals() else "")
        print(f"   å¹³å‡ã‚¨ãƒ©ãƒ¼ç‡: {avg_error_rate:.2%}" if 'avg_error_rate' in locals() else "")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {long_term_metrics['memory_leaks_detected']}å›")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®‰å®šæ€§: {throughput_stability:.1%}" if 'throughput_stability' in locals() else "")