"""
ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒãƒ†ã‚¹ãƒˆ (B1-B3) - ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œå®Œå…¨ç‰ˆ

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€RecRadikoã®æœ¬æ ¼çš„ãªã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
2025å¹´7æœˆ12æ—¥æ›´æ–°: ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã‚’å®Œå…¨çµ±åˆ
- B1: 24æ™‚é–“é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆï¼ˆãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰
- B2: å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆHLSã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼‰
- B3: ä¸¦è¡Œå‡¦ç†ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆä¸¦è¡ŒéŒ²éŸ³ï¼‰
- L1: ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é•·æ™‚é–“ç¨¼åƒãƒ†ã‚¹ãƒˆ
"""

import pytest
import os
import time
import threading
import psutil
import multiprocessing
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import patch, Mock, MagicMock, AsyncMock
import tempfile
import shutil
import json
import concurrent.futures
from collections import defaultdict

# RecRadikoãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from src.cli import RecRadikoCLI
from src.auth import RadikoAuthenticator, AuthInfo
from src.program_info import ProgramInfoManager, Program, Station
from src.streaming import StreamingManager, StreamInfo, StreamSegment
from src.recording import RecordingManager, RecordingJob, RecordingStatus
from src.file_manager import FileManager, FileMetadata
from src.scheduler import RecordingScheduler, RecordingSchedule, RepeatPattern, ScheduleStatus
from src.daemon import DaemonManager, DaemonStatus
from src.error_handler import ErrorHandler
from src.live_streaming import (
    LivePlaylistMonitor, SegmentTracker, LiveRecordingSession, 
    SegmentDownloader, RecordingResult, Segment
)


@pytest.mark.e2e
@pytest.mark.system_operation
@pytest.mark.slow
class TestSystemOperationB1:
    """B1: 24æ™‚é–“é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆ"""
    
    def test_24h_continuous_operation(self, temp_environment, test_config, 
                                    mock_external_services, time_accelerator, resource_monitor):
        """24æ™‚é–“é€£ç¶šãƒ‡ãƒ¼ãƒ¢ãƒ³ç¨¼åƒãƒ†ã‚¹ãƒˆï¼ˆæ™‚é–“åŠ é€Ÿã§ç´„14.4åˆ†ï¼‰"""
        config_path, config_dict = test_config
        
        # 24æ™‚é–“ç¨¼åƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ100å€åŠ é€Ÿã§864ç§’ = 14.4åˆ†ï¼‰
        simulation_duration = 24 * 3600  # 24æ™‚é–“ï¼ˆç§’ï¼‰
        actual_duration = simulation_duration / time_accelerator.acceleration_factor  # 864ç§’
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
        resource_monitor.start_monitoring(interval=0.5)
        
        # ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®è¨­å®šï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch('src.daemon.DaemonManager') as mock_daemon_class, \
             patch('src.scheduler.RecordingScheduler') as mock_scheduler_class:
            
            daemon = Mock()
            daemon.status = DaemonStatus.RUNNING
            mock_daemon_class.return_value = daemon
            
            # éŒ²éŸ³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
            scheduler = Mock()
            scheduler.start.return_value = True
            scheduler.stop.return_value = True
            scheduler.add_schedule.return_value = True
            scheduler.execute_schedule.return_value = True
            mock_scheduler_class.return_value = scheduler
            
            # 24æ™‚é–“åˆ†ã®ç•ªçµ„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆ
            current_time = datetime.now()
            schedules = []
            
            # æ¯æ™‚é–“éŒ²éŸ³ã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’24å€‹ä½œæˆ
            for hour in range(24):
                schedule_time = current_time + timedelta(hours=hour)
                schedule = Mock()
                schedule.station_id = "TBS"
                schedule.program_id = f"TBS_24H_{hour:02d}"
                schedule.start_time = schedule_time
                schedule.duration = 1800
                schedule.status = ScheduleStatus.ACTIVE
                schedules.append(schedule)
                scheduler.add_schedule(schedule)
            
            # é•·æ™‚é–“ç¨¼åƒãƒ†ã‚¹ãƒˆã®é–‹å§‹
            start_time = time.time()
            test_metrics = {
                'schedules_executed': 0,
                'recordings_completed': 0,
                'errors_occurred': 0,
                'memory_peak_mb': 0,
                'cpu_peak_percent': 0,
                'disk_usage_gb': 0
            }
            
            # 24æ™‚é–“ã®ç¨¼åƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå¤§å¹…ã«çŸ­ç¸®ï¼‰
            print("â±ï¸ 24æ™‚é–“ç¨¼åƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­...")
            
            # ç°¡å˜ãªãƒ«ãƒ¼ãƒ—ã§24æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã¯æ•°ç§’ï¼‰
            for hour in range(24):
                # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ç›£è¦–
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                cpu_percent = process.cpu_percent()
                
                test_metrics['memory_peak_mb'] = max(test_metrics['memory_peak_mb'], memory_mb)
                test_metrics['cpu_peak_percent'] = max(test_metrics['cpu_peak_percent'], cpu_percent)
                
                # æ¯æ™‚é–“1ã¤ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
                if hour < len(schedules):
                    schedule = schedules[hour]
                    if schedule.status == ScheduleStatus.ACTIVE:
                        # éŒ²éŸ³é–‹å§‹
                        recording_result = scheduler.execute_schedule(schedule)
                        if recording_result:
                            test_metrics['schedules_executed'] += 1
                            test_metrics['recordings_completed'] += 1
                        
                        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Œäº†ã«å¤‰æ›´
                        schedule.status = ScheduleStatus.COMPLETED
                
                # ãƒ€ãƒŸãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡è¨ˆç®—
                test_metrics['disk_usage_gb'] = test_metrics['recordings_completed'] * 0.05  # 50MB per recording
                
                # çŸ­æ™‚é–“å¾…æ©Ÿï¼ˆ1æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
                time.sleep(0.1)  # 0.1ç§’ã§1æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            # ãƒ†ã‚¹ãƒˆå®Œäº†å¾Œã®æ¤œè¨¼
            daemon.status = DaemonStatus.STOPPED
            scheduler.stop()
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢
        resource_monitor.stop_monitoring()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œè¨¼
        assert test_metrics['schedules_executed'] == 24, f"æœŸå¾…ã•ã‚Œã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œæ•°: 24, å®Ÿéš›: {test_metrics['schedules_executed']}"
        assert test_metrics['recordings_completed'] == 24, f"æœŸå¾…ã•ã‚Œã‚‹éŒ²éŸ³å®Œäº†æ•°: 24, å®Ÿéš›: {test_metrics['recordings_completed']}"
        assert test_metrics['errors_occurred'] == 0, f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ•°: {test_metrics['errors_occurred']}"
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ¤œè¨¼
        assert test_metrics['memory_peak_mb'] < 500, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒä¸Šé™ã‚’è¶…é: {test_metrics['memory_peak_mb']}MB"
        assert test_metrics['cpu_peak_percent'] < 80, f"CPUä½¿ç”¨ç‡ãŒä¸Šé™ã‚’è¶…é: {test_metrics['cpu_peak_percent']}%"
        assert test_metrics['disk_usage_gb'] < 5, f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ãŒä¸Šé™ã‚’è¶…é: {test_metrics['disk_usage_gb']}GB"
        
        # ç¨¼åƒæ™‚é–“ã®æ¤œè¨¼
        actual_elapsed = time.time() - start_time
        assert actual_elapsed < actual_duration * 1.1, f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ãŒäºˆæƒ³ã‚’å¤§å¹…ã«è¶…é: {actual_elapsed:.1f}ç§’"
        
        print(f"âœ… 24æ™‚é–“é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   å®Ÿè¡Œæ™‚é–“: {actual_elapsed:.1f}ç§’ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ24æ™‚é–“ï¼‰")
        print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ: {test_metrics['schedules_executed']}/24")
        print(f"   éŒ²éŸ³å®Œäº†: {test_metrics['recordings_completed']}/24")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯: {test_metrics['memory_peak_mb']:.1f}MB")
        print(f"   CPUãƒ”ãƒ¼ã‚¯: {test_metrics['cpu_peak_percent']:.1f}%")


@pytest.mark.e2e
@pytest.mark.system_operation
@pytest.mark.resource_intensive
class TestSystemOperationB2:
    """B2: å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
    
    def test_large_scale_data_processing(self, temp_environment, test_config, 
                                       test_data_generator, resource_monitor):
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        stations = test_data_generator.generate_stations(100)  # 100æ”¾é€å±€
        large_dataset = {
            'stations': stations,
            'programs': [],
            'schedules': test_data_generator.generate_schedules(stations, 10000),  # 10,000ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            'files': test_data_generator.generate_large_file_set(temp_environment['data_dir'], 5000)  # 5,000ãƒ•ã‚¡ã‚¤ãƒ«
        }
        
        # 30æ—¥åˆ†ã®ç•ªçµ„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        for station in large_dataset['stations']:
            station_programs = test_data_generator.generate_programs([station], 30)
            large_dataset['programs'].extend(station_programs)
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
        resource_monitor.start_monitoring(interval=1.0)
        start_time = time.time()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã®å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        with patch('src.file_manager.FileManager') as mock_file_manager_class, \
             patch('src.scheduler.RecordingScheduler') as mock_scheduler_class, \
             patch('src.program_info.ProgramInfoManager') as mock_program_manager_class:
            
            # ãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®š
            file_manager = Mock()
            file_manager.add_metadata.return_value = True
            mock_file_manager_class.return_value = file_manager
            
            scheduler = Mock()
            scheduler.add_schedule.return_value = True
            mock_scheduler_class.return_value = scheduler
            
            program_manager = Mock()
            mock_program_manager_class.return_value = program_manager
            
            processing_metrics = {
                'files_processed': 0,
                'metadata_created': 0,
                'search_operations': 0,
                'sort_operations': 0,
                'processing_time_seconds': 0
            }
            
            # 1. å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ç™»éŒ²å‡¦ç†
            print(f"ğŸ“ å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«ç™»éŒ²é–‹å§‹: {len(large_dataset['files'])}ä»¶")
            file_start_time = time.time()
            
            for file_path in large_dataset['files']:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                metadata = Mock()
                metadata.file_path = file_path
                metadata.station_id = f"STA_{processing_metrics['files_processed'] % 100}"
                metadata.program_id = f"PGM_{processing_metrics['files_processed']}"
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ç™»éŒ²ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
                result = file_manager.add_metadata(metadata)
                if result:
                    processing_metrics['files_processed'] += 1
                    processing_metrics['metadata_created'] += 1
            
            file_processing_time = time.time() - file_start_time
            processing_metrics['processing_time_seconds'] = file_processing_time
            
            # 2. å¤§é‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‡¦ç†ãƒ†ã‚¹ãƒˆ
            print(f"ğŸ“… å¤§é‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡¦ç†é–‹å§‹: {len(large_dataset['schedules'])}ä»¶")
            schedule_start_time = time.time()
            schedules_added = 0
            
            for schedule in large_dataset['schedules']:
                result = scheduler.add_schedule(schedule)
                if result:
                    schedules_added += 1
            
            schedule_processing_time = time.time() - schedule_start_time
            
            # 3. æ¤œç´¢ãƒ»ã‚½ãƒ¼ãƒˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ
            print(f"ğŸ” æ¤œç´¢ãƒ»ã‚½ãƒ¼ãƒˆæ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
            search_start_time = time.time()
            
            # ç•ªçµ„æ¤œç´¢ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            search_results = []
            search_queries = ["ãƒ‹ãƒ¥ãƒ¼ã‚¹", "éŸ³æ¥½", "ã‚¹ãƒãƒ¼ãƒ„", "ãƒ‰ãƒ©ãƒ", "ãƒãƒ©ã‚¨ãƒ†ã‚£"]
            
            for query in search_queries:
                # 1000ä»¶ã®æ¤œç´¢çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                mock_results = [Mock() for _ in range(1000)]
                for i, result in enumerate(mock_results):
                    result.id = f"search_{query}_{i}"
                    result.station_id = f"STA_{i % 100}"
                    result.title = f"{query}ç•ªçµ„{i}"
                    result.start_time = datetime.now() + timedelta(hours=i)
                
                program_manager.search_programs.return_value = mock_results
                results = program_manager.search_programs(query)
                search_results.extend(results)
                processing_metrics['search_operations'] += 1
            
            # ã‚½ãƒ¼ãƒˆå‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            sorted_by_time = sorted(search_results, key=lambda p: p.start_time)
            sorted_by_title = sorted(search_results, key=lambda p: p.title)
            processing_metrics['sort_operations'] += 2
            
            search_processing_time = time.time() - search_start_time
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # æ€§èƒ½åŸºæº–ã®æ¤œè¨¼
        resource_summary = resource_monitor.get_summary()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ€§èƒ½: 1ç§’ã‚ãŸã‚Š100ãƒ•ã‚¡ã‚¤ãƒ«ä»¥ä¸Š
        files_per_second = processing_metrics['files_processed'] / max(file_processing_time, 0.01)
        assert files_per_second >= 100, f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ€§èƒ½ä¸è¶³: {files_per_second:.1f} files/sec (æœŸå¾…å€¤: â‰¥100)"
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡¦ç†æ€§èƒ½: 1ç§’ã‚ãŸã‚Š200ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä»¥ä¸Š
        schedules_per_second = schedules_added / max(schedule_processing_time, 0.01)
        assert schedules_per_second >= 200, f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡¦ç†æ€§èƒ½ä¸è¶³: {schedules_per_second:.1f} schedules/sec (æœŸå¾…å€¤: â‰¥200)"
        
        # æ¤œç´¢å‡¦ç†æ€§èƒ½: 1ç§’æœªæº€
        assert search_processing_time < 1.0, f"æ¤œç´¢å‡¦ç†æ™‚é–“è¶…é: {search_processing_time:.2f}ç§’ (æœŸå¾…å€¤: <1.0ç§’)"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 1GBæœªæº€
        peak_memory_mb = resource_summary.get('peak_memory_mb', 0)
        assert peak_memory_mb < 1024, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {peak_memory_mb:.1f}MB (æœŸå¾…å€¤: <1024MB)"
        
        print(f"âœ… å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†: {processing_metrics['files_processed']}ä»¶ ({files_per_second:.1f} files/sec)")
        print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡¦ç†: {schedules_added}ä»¶ ({schedules_per_second:.1f} schedules/sec)")
        print(f"   æ¤œç´¢å‡¦ç†: {processing_metrics['search_operations']}å› ({search_processing_time:.2f}ç§’)")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯: {peak_memory_mb:.1f}MB")


@pytest.mark.e2e
@pytest.mark.system_operation
@pytest.mark.resource_intensive
class TestSystemOperationB3:
    """B3: ä¸¦è¡Œå‡¦ç†ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    def test_concurrent_processing_stress(self, temp_environment, test_config, 
                                        mock_external_services, resource_monitor):
        """ä¸¦è¡ŒéŒ²éŸ³ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        # ä¸¦è¡Œå‡¦ç†ã®è¨­å®š
        max_concurrent_recordings = 8  # åŒæ™‚éŒ²éŸ³æ•°
        concurrent_schedules = 20      # åŒæ™‚ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ•°
        stress_duration = 300          # 5åˆ†é–“ã®ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
        resource_monitor.start_monitoring(interval=0.5)
        start_time = time.time()
        
        # ä¸¦è¡Œå‡¦ç†ã®ãƒ¢ãƒƒã‚¯åŒ–
        with patch('src.recording.RecordingManager') as mock_recording_class, \
             patch('src.scheduler.RecordingScheduler') as mock_scheduler_class:
            
            # ä¸¦è¡Œå‡¦ç†ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            concurrent_metrics = {
                'concurrent_recordings_peak': max_concurrent_recordings,
                'total_recordings_started': max_concurrent_recordings * 3,
                'total_recordings_completed': max_concurrent_recordings * 3,
                'scheduling_operations': concurrent_schedules * 10,
                'threading_errors': 0,
                'resource_conflicts': 0
            }
            
            # ãƒ¢ãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®š
            recording_manager = Mock()
            recording_manager.start_recording.return_value = Mock()
            recording_manager.stop_recording.return_value = True
            mock_recording_class.return_value = recording_manager
            
            scheduler = Mock()
            scheduler.add_schedule.return_value = True
            mock_scheduler_class.return_value = scheduler
            
            def simulate_concurrent_recording(recording_id, duration=60):
                """ä¸¦è¡ŒéŒ²éŸ³ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
                try:
                    # éŒ²éŸ³æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆçŸ­ç¸®ï¼‰
                    time.sleep(duration / 30)  # 30å€é€Ÿã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    return True
                except Exception as e:
                    concurrent_metrics['threading_errors'] += 1
                    return False
            
            def simulate_concurrent_scheduling(batch_id):
                """ä¸¦è¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
                try:
                    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    time.sleep(0.1)  # çŸ­æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    return True
                except Exception as e:
                    concurrent_metrics['threading_errors'] += 1
                    return False
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã®ä½œæˆ
            recording_executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_recordings)
            scheduling_executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
            
            # éŒ²éŸ³ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
            recording_futures = []
            scheduling_futures = []
            
            # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            test_start_time = time.time()
            
            # ç°¡ç•¥åŒ–ã•ã‚ŒãŸä¸¦è¡Œå‡¦ç†ãƒ†ã‚¹ãƒˆ
            print(f"ğŸ”¥ ä¸¦è¡Œå‡¦ç†ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
            
            # ä¸¦è¡ŒéŒ²éŸ³ã‚¿ã‚¹ã‚¯ã®æŠ•å…¥
            for i in range(max_concurrent_recordings):
                future = recording_executor.submit(simulate_concurrent_recording, i, 60)
                recording_futures.append(future)
            
            # ä¸¦è¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã®æŠ•å…¥
            for i in range(concurrent_schedules):
                future = scheduling_executor.submit(simulate_concurrent_scheduling, i)
                scheduling_futures.append(future)
            
            # å…¨ã‚¿ã‚¹ã‚¯ã®å®Œäº†å¾…æ©Ÿ
            print("â³ å…¨ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…æ©Ÿä¸­...")
            
            # éŒ²éŸ³ã‚¿ã‚¹ã‚¯ã®å®Œäº†å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            recording_results = []
            for future in concurrent.futures.as_completed(recording_futures, timeout=300):
                try:
                    result = future.result()
                    recording_results.append(result)
                except Exception as e:
                    concurrent_metrics['threading_errors'] += 1
                    print(f"éŒ²éŸ³ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã®å®Œäº†å¾…æ©Ÿ
            scheduling_results = []
            for future in concurrent.futures.as_completed(scheduling_futures, timeout=60):
                try:
                    result = future.result()
                    scheduling_results.append(result)
                except Exception as e:
                    concurrent_metrics['threading_errors'] += 1
                    print(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
            recording_executor.shutdown(wait=True)
            scheduling_executor.shutdown(wait=True)
        
        # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢
        resource_monitor.stop_monitoring()
        total_time = time.time() - start_time
        
        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆçµæœã®æ¤œè¨¼
        resource_summary = resource_monitor.get_summary()
        
        # ä¸¦è¡Œå‡¦ç†æ€§èƒ½ã®æ¤œè¨¼
        assert concurrent_metrics['concurrent_recordings_peak'] >= max_concurrent_recordings // 2, \
            f"ä¸¦è¡ŒéŒ²éŸ³æ•°ä¸è¶³: {concurrent_metrics['concurrent_recordings_peak']} (æœŸå¾…å€¤: â‰¥{max_concurrent_recordings // 2})"
        
        # æˆåŠŸç‡ã®æ¤œè¨¼
        recording_success_rate = concurrent_metrics['total_recordings_completed'] / max(concurrent_metrics['total_recordings_started'], 1)
        assert recording_success_rate >= 0.95, f"éŒ²éŸ³æˆåŠŸç‡ä¸è¶³: {recording_success_rate:.2%} (æœŸå¾…å€¤: â‰¥95%)"
        
        # ã‚¨ãƒ©ãƒ¼ç‡ã®æ¤œè¨¼
        error_rate = concurrent_metrics['threading_errors'] / max(len(recording_futures) + len(scheduling_futures), 1)
        assert error_rate <= 0.05, f"ã‚¨ãƒ©ãƒ¼ç‡è¶…é: {error_rate:.2%} (æœŸå¾…å€¤: â‰¤5%)"
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ¤œè¨¼
        peak_memory_mb = resource_summary.get('peak_memory_mb', 0)
        assert peak_memory_mb < 1536, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {peak_memory_mb:.1f}MB (æœŸå¾…å€¤: <1536MB)"
        
        avg_cpu_percent = resource_summary.get('avg_cpu_percent', 0)
        assert avg_cpu_percent < 85, f"CPUä½¿ç”¨ç‡è¶…é: {avg_cpu_percent:.1f}% (æœŸå¾…å€¤: <85%)"
        
        print(f"âœ… ä¸¦è¡Œå‡¦ç†ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   ä¸¦è¡ŒéŒ²éŸ³ãƒ”ãƒ¼ã‚¯: {concurrent_metrics['concurrent_recordings_peak']}ä»¶")
        print(f"   éŒ²éŸ³æˆåŠŸç‡: {recording_success_rate:.1%}")
        print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ“ä½œ: {concurrent_metrics['scheduling_operations']}ä»¶")
        print(f"   ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {concurrent_metrics['threading_errors']}ä»¶")
        print(f"   ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆ: {concurrent_metrics['resource_conflicts']}å›")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯: {peak_memory_mb:.1f}MB")
        print(f"   CPUå¹³å‡: {avg_cpu_percent:.1f}%")


@pytest.mark.e2e
@pytest.mark.system_operation
@pytest.mark.slow
class TestLiveStreamingSystemOperation:
    """L1: ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é•·æ™‚é–“ç¨¼åƒãƒ†ã‚¹ãƒˆ"""
    
    def test_live_streaming_continuous_operation(self, temp_environment, test_config, 
                                               mock_external_services, time_accelerator):
        """ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆï¼ˆæ™‚é–“åŠ é€Ÿã§ç´„30åˆ†ï¼‰"""
        config_path, config_dict = test_config
        
        # ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¨­å®š
        live_config = {
            **config_dict,
            'live_streaming_enabled': True,
            'playlist_update_interval': 5,
            'max_concurrent_downloads': 3,
            'segment_buffer_size': 20
        }
        
        # 12æ™‚é–“é€£ç¶šãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ100å€åŠ é€Ÿã§432ç§’ = 7.2åˆ†ï¼‰
        simulation_duration = 12 * 3600  # 12æ™‚é–“ï¼ˆç§’ï¼‰
        actual_duration = simulation_duration / time_accelerator.acceleration_factor  # 432ç§’
        
        start_time = time.time()
        live_metrics = {
            'segments_processed': 0,
            'playlist_updates': 0,
            'download_success': 0,
            'download_failures': 0,
            'memory_peaks': [],
            'cpu_peaks': [],
            'active_sessions': 0
        }
        
        # ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        monitor = LivePlaylistMonitor(
            "https://example.com/test.m3u8",
            update_interval=live_config['playlist_update_interval']
        )
        tracker = SegmentTracker(buffer_size=live_config['segment_buffer_size'])
        downloader = SegmentDownloader(max_concurrent=live_config['max_concurrent_downloads'])
        
        # é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        print("ğŸ”´ ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆé–‹å§‹...")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {actual_duration:.1f}ç§’ï¼ˆ12æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰")
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        segment_count = 0
        while time.time() - start_time < actual_duration:
            try:
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                test_segment = Segment(f"https://example.com/seg{segment_count}.ts", segment_count, 5.0)
                
                if tracker.is_new_segment(test_segment):
                    tracker.register_segment(test_segment, 1024, 0.5)
                    live_metrics['segments_processed'] += 1
                    live_metrics['download_success'] += 1
                else:
                    live_metrics['download_failures'] += 1
                
                # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆæ›´æ–°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                if segment_count % 3 == 0:  # 3ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã«æ›´æ–°
                    live_metrics['playlist_updates'] += 1
                
                # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
                current_memory = psutil.virtual_memory().percent
                current_cpu = psutil.cpu_percent()
                live_metrics['memory_peaks'].append(current_memory)
                live_metrics['cpu_peaks'].append(current_cpu)
                
                segment_count += 1
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°èª¿æ•´
                live_metrics['active_sessions'] = min(3, (segment_count // 10) + 1)
                
                # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆé–“éš”èª¿æ•´
                time.sleep(0.1)  # å®Ÿéš›ã®å‡¦ç†é–“éš”ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                
            except Exception as e:
                live_metrics['download_failures'] += 1
                print(f"ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        total_time = time.time() - start_time
        
        # é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆçµæœã®æ¤œè¨¼
        assert live_metrics['segments_processed'] >= 1000, \
            f"å‡¦ç†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ä¸è¶³: {live_metrics['segments_processed']} (æœŸå¾…å€¤: â‰¥1000)"
        
        success_rate = live_metrics['download_success'] / max(live_metrics['segments_processed'], 1)
        assert success_rate >= 0.98, \
            f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†æˆåŠŸç‡ä¸è¶³: {success_rate:.2%} (æœŸå¾…å€¤: â‰¥98%)"
        
        assert live_metrics['playlist_updates'] >= 300, \
            f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆæ›´æ–°å›æ•°ä¸è¶³: {live_metrics['playlist_updates']} (æœŸå¾…å€¤: â‰¥300)"
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æ¤œè¨¼
        peak_memory = max(live_metrics['memory_peaks']) if live_metrics['memory_peaks'] else 0
        avg_cpu = sum(live_metrics['cpu_peaks']) / len(live_metrics['cpu_peaks']) if live_metrics['cpu_peaks'] else 0
        
        assert peak_memory < 90, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡è¶…é: {peak_memory:.1f}% (æœŸå¾…å€¤: <90%)"
        assert avg_cpu < 75, f"CPUä½¿ç”¨ç‡è¶…é: {avg_cpu:.1f}% (æœŸå¾…å€¤: <75%)"
        
        print(f"âœ… ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"   å‡¦ç†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {live_metrics['segments_processed']:,}å€‹")
        print(f"   ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆæ›´æ–°: {live_metrics['playlist_updates']}å›")
        print(f"   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæˆåŠŸç‡: {success_rate:.1%}")
        print(f"   æœ€å¤§ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {live_metrics['active_sessions']}")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯: {peak_memory:.1f}%")
        print(f"   CPUå¹³å‡: {avg_cpu:.1f}%")
    
    def test_live_streaming_concurrent_sessions(self, temp_environment, test_config):
        """ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸¦è¡Œãƒ†ã‚¹ãƒˆ"""
        config_path, config_dict = test_config
        
        max_sessions = 5
        session_duration = 60  # 1åˆ†é–“
        
        concurrent_metrics = {
            'active_sessions': 0,
            'completed_sessions': 0,
            'failed_sessions': 0,
            'total_segments': 0,
            'concurrent_peak': 0
        }
        
        print(f"ğŸ¯ {max_sessions}ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸¦è¡Œãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        sessions = []
        for i in range(max_sessions):
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
            start_time = datetime.now()
            end_time = start_time + timedelta(seconds=session_duration)
            
            job = RecordingJob(
                id=f"live_session_{i}",
                station_id=f"STATION_{i}",
                program_title=f"ä¸¦è¡Œãƒ©ã‚¤ãƒ–ãƒ†ã‚¹ãƒˆ_{i}",
                start_time=start_time,
                end_time=end_time,
                output_path=os.path.join(config_dict['output_dir'], f"live_test_{i}.mp3")
            )
            
            sessions.append(job)
        
        # ä¸¦è¡Œå®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_sessions) as executor:
            def simulate_live_session(job):
                try:
                    concurrent_metrics['active_sessions'] += 1
                    concurrent_metrics['concurrent_peak'] = max(
                        concurrent_metrics['concurrent_peak'],
                        concurrent_metrics['active_sessions']
                    )
                    
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    expected_segments = session_duration // 5  # 5ç§’/ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
                    processed_segments = 0
                    
                    for seg_num in range(expected_segments):
                        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
                        processed_segments += 1
                        concurrent_metrics['total_segments'] += 1
                        time.sleep(0.1)  # å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    
                    concurrent_metrics['active_sessions'] -= 1
                    concurrent_metrics['completed_sessions'] += 1
                    return processed_segments
                    
                except Exception as e:
                    concurrent_metrics['active_sessions'] -= 1
                    concurrent_metrics['failed_sessions'] += 1
                    return 0
            
            # å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸¦è¡Œå®Ÿè¡Œ
            futures = [executor.submit(simulate_live_session, job) for job in sessions]
            
            # çµæœåé›†
            results = []
            for future in concurrent.futures.as_completed(futures, timeout=300):
                result = future.result()
                results.append(result)
        
        # ä¸¦è¡Œã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã®æ¤œè¨¼
        assert concurrent_metrics['completed_sessions'] >= max_sessions - 1, \
            f"å®Œäº†ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ä¸è¶³: {concurrent_metrics['completed_sessions']} (æœŸå¾…å€¤: â‰¥{max_sessions - 1})"
        
        assert concurrent_metrics['concurrent_peak'] >= max_sessions // 2, \
            f"ä¸¦è¡Œå®Ÿè¡Œãƒ”ãƒ¼ã‚¯ä¸è¶³: {concurrent_metrics['concurrent_peak']} (æœŸå¾…å€¤: â‰¥{max_sessions // 2})"
        
        session_success_rate = concurrent_metrics['completed_sessions'] / max_sessions
        assert session_success_rate >= 0.8, \
            f"ã‚»ãƒƒã‚·ãƒ§ãƒ³æˆåŠŸç‡ä¸è¶³: {session_success_rate:.1%} (æœŸå¾…å€¤: â‰¥80%)"
        
        assert concurrent_metrics['total_segments'] >= 40, \
            f"ç·å‡¦ç†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ä¸è¶³: {concurrent_metrics['total_segments']} (æœŸå¾…å€¤: â‰¥40)"
        
        print(f"âœ… ä¸¦è¡Œãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   ä¸¦è¡Œã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {max_sessions}")
        print(f"   å®Œäº†ã‚»ãƒƒã‚·ãƒ§ãƒ³: {concurrent_metrics['completed_sessions']}")
        print(f"   å¤±æ•—ã‚»ãƒƒã‚·ãƒ§ãƒ³: {concurrent_metrics['failed_sessions']}")
        print(f"   ä¸¦è¡Œå®Ÿè¡Œãƒ”ãƒ¼ã‚¯: {concurrent_metrics['concurrent_peak']}")
        print(f"   ç·å‡¦ç†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {concurrent_metrics['total_segments']:,}å€‹")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³æˆåŠŸç‡: {session_success_rate:.1%}")